{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic chat stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.tools import StructuredTool, ToolException\n",
    "from typing import List, Annotated\n",
    "from langchain.agents import load_tools\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import ToolNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grobid xml processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = fitz.open(os.path.join('/home/aditya-ladawa/Aditya/z_projects/aireas/api/static/files/', 'a.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aditya-ladawa/Aditya/z_projects/aireas/api'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "\t\t\t\t\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need\n",
      "\n",
      "\t\t\t\t\n",
      "None\n",
      "\n",
      "\t\t\t\t\t\n",
      "None\n",
      "2 Aug 2023\n",
      "\n",
      "\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Ashish\n",
      "Vaswani\n",
      "avaswani@google.com\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Noam\n",
      "Shazeer\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Google\n",
      "Brain\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Niki\n",
      "Parmar\n",
      "nikip@google.com\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Jakob\n",
      "Uszkoreit\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Llion\n",
      "Jones\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Aidan\n",
      "N\n",
      "Gomez\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "None\n",
      "Łukasz\n",
      "Kaiser\n",
      "lukaszkaiser@google.com\n",
      "\n",
      "\t\t\t\t\t\t\t\t\n",
      "Google Brain Google Research Google Research Google Research University of Toronto Google Brain 31st Conference on Neural Information Processing Systems (NIPS 2017) , Long Beach , CA , USA.\n",
      "Google Brain\n",
      "Google Research\n",
      "Google Research\n",
      "Google Research\n",
      "Google Brain\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017)\n",
      "University of Toronto\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "Long Beach\n",
      "CA\n",
      "USA\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need\n",
      "\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\t\n",
      "2 Aug 2023\n",
      "18E1B007A1DAB45B30CC861BA2DFDA25\n",
      "arXiv:1706.03762v7[cs.CL]\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\n",
      "GROBID - A machine learning software for extracting information from scholarly documents\n",
      "None\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "* Equal contribution.\n",
      "Listing order is random.\n",
      "Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n",
      "Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work.\n",
      "Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.\n",
      "Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.\n",
      "Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.\n",
      "Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "None\n",
      "† Work performed while at Google Brain.\n",
      "None\n",
      "‡ Work performed while at Google Research.\n",
      "\n",
      "\t\t\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "None\n",
      "Introduction\n",
      "None\n",
      "Recurrent neural networks, long short-term memory \n",
      "[13]\n",
      "[7]\n",
      "[35,\n",
      "2,\n",
      "5]\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures \n",
      "[38,\n",
      "24,\n",
      "15]\n",
      "None\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t-1 and the input for position t.\n",
      "This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks \n",
      "[21]\n",
      "[32]\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "None\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences \n",
      "[2,\n",
      "19]\n",
      "In all but a few cases \n",
      "[27]\n",
      "None\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "None\n",
      "Background\n",
      "None\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU \n",
      "[16]\n",
      "[18]\n",
      "[9]\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions \n",
      "[12]\n",
      "In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "None\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations \n",
      "[4,\n",
      "27,\n",
      "28,\n",
      "22]\n",
      "None\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks \n",
      "[34]\n",
      "None\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as \n",
      "[17,\n",
      "18]\n",
      "[9]\n",
      "None\n",
      "Model Architecture\n",
      "None\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure \n",
      "[5,\n",
      "2,\n",
      "35]\n",
      "Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ).\n",
      "Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time.\n",
      "At each step the model is auto-regressive \n",
      "[10]\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure \n",
      "1\n",
      "None\n",
      "Encoder and Decoder Stacks\n",
      "None\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers.\n",
      "Each layer has two sub-layers.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.\n",
      "We employ a residual connection \n",
      "[11]\n",
      "[1]\n",
      "That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512.\n",
      "None\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "None\n",
      "Attention\n",
      "None\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "The output is computed as a weighted sum Scaled Dot-Product Attention Multi-Head Attention of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "None\n",
      "Scaled Dot-Product Attention\n",
      "None\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure \n",
      "2\n",
      "The input consists of queries and keys of dimension d k , and values of dimension d v .\n",
      "We compute the dot products of the query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.\n",
      "None\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The keys and values are also packed together into matrices K and V .\n",
      "We compute the matrix of outputs as:\n",
      "Attention(Q, K, V ) = softmax( QK T √ d k )V\n",
      "(1)\n",
      "None\n",
      "The two most commonly used attention functions are additive attention \n",
      "[2]\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1\n",
      "√ d k\n",
      "None\n",
      ". Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "None\n",
      "While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k \n",
      "[3]\n",
      "We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients\n",
      "foot_0\n",
      "To counteract this effect, we scale the dot products by 1\n",
      "√ d k .\n",
      "None\n",
      "Multi-Head Attention\n",
      "None\n",
      "Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively.\n",
      "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values.\n",
      "These are concatenated and once again projected, resulting in the final values, as depicted in Figure \n",
      "2\n",
      "None\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O\n",
      "None\n",
      "where\n",
      "head i = Attention(QW Q i , KW K i , V W V i )\n",
      "None\n",
      "Where the projections are parameter matrices\n",
      "W Q i ∈ R dmodel×d k , W K i ∈ R dmodel×d k , W V i ∈ R dmodel×dv and W O ∈ R hdv×dmodel .\n",
      "None\n",
      "In this work we employ h = 8 parallel attention layers, or heads.\n",
      "For each of these we use\n",
      "d k = d v = d model /h = 64.\n",
      "None\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "None\n",
      "Applications of Attention in our Model\n",
      "None\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "None\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as \n",
      "[38,\n",
      "2,\n",
      "9]\n",
      "None\n",
      "• The encoder contains self-attention layers.\n",
      "In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "None\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to -∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "See Figure \n",
      "2\n",
      "None\n",
      "Position-wise Feed-Forward Networks\n",
      "None\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2\n",
      "(2)\n",
      "None\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
      "Another way of describing this is as two convolutions with kernel size 1.\n",
      "None\n",
      "The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.\n",
      "None\n",
      "Embeddings and Softmax\n",
      "None\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to \n",
      "[30]\n",
      "In the embedding layers, we multiply those weights by √ d model .\n",
      "(n 2 • d) O(1) O(1) Recurrent O(n • d 2 ) O(n) O(n) Convolutional O(k • n • d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r • n • d) O(1) O(n/r)\n",
      "None\n",
      "Positional Encoding\n",
      "None\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension d model as the embeddings, so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed \n",
      "[9]\n",
      "None\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "P E (pos,2i) = sin(pos/10000 2i/dmodel ) P E (pos,2i+1) = cos(pos/10000 2i/dmodel )\n",
      "None\n",
      "where pos is the position and i is the dimension.\n",
      "That is, each dimension of the positional encoding corresponds to a sinusoid.\n",
      "The wavelengths form a geometric progression from 2π to 10000 • 2π.\n",
      "We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos .\n",
      "None\n",
      "We also experimented with using learned positional embeddings \n",
      "[9]\n",
      "3\n",
      "We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "None\n",
      "Why Self-Attention\n",
      "None\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "None\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "None\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies \n",
      "[12]\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "None\n",
      "As noted in Table \n",
      "1\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece \n",
      "[38]\n",
      "[31]\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "This would increase the maximum path length to O(n/r).\n",
      "We plan to investigate this approach further in future work.\n",
      "None\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions.\n",
      "Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions \n",
      "[18]\n",
      "Convolutional layers are generally more expensive than recurrent layers, by a factor of k.\n",
      "Separable convolutions \n",
      "[6]\n",
      "• n • d + n • d 2 ).\n",
      "None\n",
      "Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "None\n",
      "As side benefit, self-attention could yield more interpretable models.\n",
      "We inspect attention distributions from our models and present and discuss examples in the appendix.\n",
      "Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "None\n",
      "Training\n",
      "None\n",
      "This section describes the training regime for our models.\n",
      "None\n",
      "Training Data and Batching\n",
      "None\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding \n",
      "[3]\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary \n",
      "[38]\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "None\n",
      "Hardware and Schedule\n",
      "None\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models,(described on the bottom line of table \n",
      "3\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "None\n",
      "Optimizer\n",
      "None\n",
      "We used the Adam optimizer \n",
      "[20]\n",
      "We varied the learning rate over the course of training, according to the formula:\n",
      "lrate = d -0.5 model • min(step_num -0.5 , step_num • warmup_steps -1.5 )\n",
      "(3)\n",
      "None\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.\n",
      "We used warmup_steps = 4000.\n",
      "None\n",
      "Regularization\n",
      "None\n",
      "We employ three types of regularization during training: Residual Dropout We apply dropout \n",
      "[33]\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "For the base model, we use a rate of P drop = 0.1.\n",
      "None\n",
      "Label Smoothing\n",
      "None\n",
      "During training, we employed label smoothing of value ϵ ls = 0.1 \n",
      "[36]\n",
      "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "None\n",
      "6 Results\n",
      "None\n",
      "Machine Translation\n",
      "None\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table \n",
      "2\n",
      "The configuration of this model is listed in the bottom line of Table \n",
      "3\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "None\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3.\n",
      "None\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6 \n",
      "[38]\n",
      "These hyperparameters were chosen after experimentation on the development set.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible \n",
      "[38]\n",
      "None\n",
      "Table \n",
      "2\n",
      "We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU\n",
      "foot_1\n",
      "None\n",
      "Model Variations\n",
      "None\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the .\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "We present these results in Table \n",
      "3\n",
      "None\n",
      "In Table \n",
      "3\n",
      "While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "None\n",
      "In Table \n",
      "3\n",
      "This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial.\n",
      "We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting.\n",
      "In row (E) we replace our sinusoidal positional encoding with learned positional embeddings \n",
      "[9]\n",
      "None\n",
      "English Constituency Parsing\n",
      "None\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes \n",
      "[37]\n",
      "None\n",
      "We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank \n",
      "[25]\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences \n",
      "[37]\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "None\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model.\n",
      "During inference, we increased the maximum output length to input length + 300.\n",
      "We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "None\n",
      "Our results in Table \n",
      "4\n",
      "[8]\n",
      "None\n",
      "In contrast to RNN sequence-to-sequence models \n",
      "[37]\n",
      "[29]\n",
      "None\n",
      "Conclusion\n",
      "None\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "None\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task our best model outperforms even all previously reported ensembles.\n",
      "None\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
      "Making generation less sequential is another research goals of ours.\n",
      "None\n",
      "The code we used to train and evaluate our models is available at \n",
      "https://github.com/ tensorflow/tensor2tensor\n",
      "None\n",
      "Input-Input Layer5\n",
      "None\n",
      "The  Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6.\n",
      "Note that the attentions are very sharp for this word.\n",
      "None\n",
      "Input-Input Layer5\n",
      "None\n",
      "The\n",
      "None\n",
      "Figure 1 :\n",
      "1\n",
      "None\n",
      "None\n",
      "None\n",
      "Figure 1: The Transformer -model architecture.\n",
      "None\n",
      "None\n",
      "Figure 2 :\n",
      "2\n",
      "None\n",
      "None\n",
      "None\n",
      "Figure 2: (left) Scaled Dot-Product Attention.\n",
      "(right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "None\n",
      "None\n",
      "Figure 3 :\n",
      "3\n",
      "None\n",
      "None\n",
      "None\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.\n",
      "Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'.\n",
      "Attentions here shown only for the word 'making'.\n",
      "Different colors represent different heads.\n",
      "Best viewed in color.\n",
      "None\n",
      "Figure 4 :\n",
      "4\n",
      "None\n",
      "None\n",
      "None\n",
      "Figure4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.\n",
      "Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6.\n",
      "Note that the attentions are very sharp for this word.\n",
      "None\n",
      "Figure 5 :\n",
      "5\n",
      "None\n",
      "None\n",
      "None\n",
      "Figure5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence.\n",
      "We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6.\n",
      "The heads clearly learned to perform different tasks.\n",
      "None\n",
      "Table 1 :\n",
      "1\n",
      "None\n",
      "None\n",
      "None\n",
      "Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.\n",
      "n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "None\n",
      "None\n",
      "Layer Type\n",
      "Complexity per Layer Sequential Maximum Path Length\n",
      "None\n",
      "None\n",
      "Operations\n",
      "None\n",
      "Self-Attention\n",
      "O\n",
      "None\n",
      "Table 2 :\n",
      "2\n",
      "None\n",
      "None\n",
      "None\n",
      "The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "None\n",
      "None\n",
      "Model\n",
      "BLEU EN-DE EN-FR\n",
      "Training Cost (FLOPs) EN-DE EN-FR\n",
      "None\n",
      "ByteNet [18]\n",
      "23.75\n",
      "None\n",
      "None\n",
      "None\n",
      "Deep-Att + PosUnk [39]\n",
      "None\n",
      "39.2\n",
      "1.0 • 10 20\n",
      "None\n",
      "GNMT + RL [38]\n",
      "24.6\n",
      "39.92\n",
      "2.3 • 10 19 1.4 • 10 20\n",
      "None\n",
      "ConvS2S [9]\n",
      "25.16\n",
      "40.46\n",
      "9.6 • 10 18 1.5 • 10 20\n",
      "None\n",
      "MoE [32]\n",
      "26.03\n",
      "40.56\n",
      "2.0 • 10 19 1.2 • 10 20\n",
      "None\n",
      "Deep-Att + PosUnk Ensemble [39]\n",
      "None\n",
      "40.4\n",
      "8.0 • 10 20\n",
      "None\n",
      "GNMT + RL Ensemble [38]\n",
      "26.30\n",
      "41.16\n",
      "1.8 • 10 20 1.1 • 10 21\n",
      "None\n",
      "ConvS2S Ensemble [9]\n",
      "26.36\n",
      "41.29\n",
      "7.7 • 10 19 1.2 • 10 21\n",
      "None\n",
      "Transformer (base model)\n",
      "27.3\n",
      "38.1\n",
      "3.3 • 10 18\n",
      "None\n",
      "Transformer (big)\n",
      "28.4\n",
      "41.8\n",
      "2.3 • 10 19\n",
      "None\n",
      "Table 3 :\n",
      "3\n",
      "None\n",
      "None\n",
      "None\n",
      "Variations on the Transformer architecture.\n",
      "Unlisted values are identical to those of the base model.\n",
      "All metrics are on the English-to-German translation development set, newstest2013.\n",
      "Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "None\n",
      "None\n",
      "None\n",
      "N d model\n",
      "d ff\n",
      "h\n",
      "d k\n",
      "d v\n",
      "P drop ϵ ls\n",
      "train steps (dev) (dev) PPL BLEU params ×10 6\n",
      "None\n",
      "base 6\n",
      "512\n",
      "2048 8\n",
      "64\n",
      "64\n",
      "0.1\n",
      "0.1 100K 4.92\n",
      "25.8\n",
      "65\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "1 512 512\n",
      "None\n",
      "None\n",
      "5.29\n",
      "24.9\n",
      "None\n",
      "(A)\n",
      "None\n",
      "None\n",
      "None\n",
      "4 128 128 16 32 32\n",
      "None\n",
      "None\n",
      "5.00 4.91\n",
      "25.5 25.8\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "32 16\n",
      "16\n",
      "None\n",
      "None\n",
      "5.01\n",
      "25.4\n",
      "None\n",
      "(B)\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "16 32\n",
      "None\n",
      "None\n",
      "None\n",
      "5.16 5.01\n",
      "25.1 25.4\n",
      "58 60\n",
      "None\n",
      "None\n",
      "2\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "6.11\n",
      "23.7\n",
      "36\n",
      "None\n",
      "None\n",
      "4\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "5.19\n",
      "25.3\n",
      "50\n",
      "None\n",
      "None\n",
      "8\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "4.88\n",
      "25.5\n",
      "80\n",
      "None\n",
      "(C)\n",
      "None\n",
      "256\n",
      "None\n",
      "None\n",
      "32\n",
      "32\n",
      "None\n",
      "None\n",
      "5.75\n",
      "24.5\n",
      "28\n",
      "None\n",
      "None\n",
      "None\n",
      "1024\n",
      "None\n",
      "None\n",
      "128 128\n",
      "None\n",
      "None\n",
      "4.66\n",
      "26.0\n",
      "168\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "1024\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "5.12\n",
      "25.4\n",
      "53\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "4096\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "4.75\n",
      "26.2\n",
      "90\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "0.0\n",
      "None\n",
      "5.77\n",
      "24.6\n",
      "None\n",
      "(D)\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "0.2\n",
      "0.0\n",
      "4.95 4.67\n",
      "25.5 25.3\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "0.2\n",
      "5.47\n",
      "25.7\n",
      "None\n",
      "(E)\n",
      "None\n",
      "positional embedding instead of sinusoids\n",
      "4.92\n",
      "25.7\n",
      "None\n",
      "big\n",
      "6\n",
      "1024 4096 16\n",
      "None\n",
      "None\n",
      "0.3\n",
      "None\n",
      "300K 4.33\n",
      "26.4\n",
      "213\n",
      "None\n",
      "development set, newstest2013\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Table 4 :\n",
      "4\n",
      "None\n",
      "None\n",
      "None\n",
      "The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "None\n",
      "None\n",
      "Parser\n",
      "Training\n",
      "WSJ 23 F1\n",
      "None\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative\n",
      "88.3\n",
      "None\n",
      "Petrov et al. (2006) [29]\n",
      "WSJ only, discriminative\n",
      "90.4\n",
      "None\n",
      "Zhu et al. (2013) [40]\n",
      "WSJ only, discriminative\n",
      "90.4\n",
      "None\n",
      "Dyer et al. (2016) [8]\n",
      "WSJ only, discriminative\n",
      "91.7\n",
      "None\n",
      "Transformer (4 layers)\n",
      "WSJ only, discriminative\n",
      "91.3\n",
      "None\n",
      "Zhu et al. (2013) [40]\n",
      "semi-supervised\n",
      "91.3\n",
      "None\n",
      "Huang & Harper (2009) [14]\n",
      "semi-supervised\n",
      "91.3\n",
      "None\n",
      "McClosky et al. (2006) [26]\n",
      "semi-supervised\n",
      "92.1\n",
      "None\n",
      "Vinyals & Kaiser el al. (2014) [37]\n",
      "semi-supervised\n",
      "92.1\n",
      "None\n",
      "Transformer (4 layers)\n",
      "semi-supervised\n",
      "92.7\n",
      "None\n",
      "Luong et al. (2015) [23]\n",
      "multi-task\n",
      "93.0\n",
      "None\n",
      "Dyer et al. (2016) [8]\n",
      "generative\n",
      "93.3\n",
      "None\n",
      "None\n",
      "To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1.\n",
      "Then their dot product, q • k = d k i=1 qiki, has mean 0 and variance d k .\n",
      "None\n",
      "None\n",
      "We used values of 2.8, 3.7,\n",
      "None\n",
      "None\n",
      "None\n",
      "6\n",
      "\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "Acknowledgements We are grateful to \n",
      "Nal Kalchbrenner\n",
      "Stephan Gouws\n",
      "\n",
      "\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "None\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jimmy\n",
      "Lei Ba\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jamie\n",
      "Ryan Kiros\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Geoffrey\n",
      "E\n",
      "Hinton\n",
      "arXiv:1607.06450\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Layer normalization. arXiv preprint\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Neural machine translation by jointly learning to align and translate\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Dzmitry\n",
      "Bahdanau\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Kyunghyun\n",
      "Cho\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoshua\n",
      "Bengio\n",
      "CoRR, abs/1409.0473\n",
      "\n",
      "\t\t\t\n",
      "2014\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Massive exploration of neural machine translation architectures\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Denny\n",
      "Britz\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Anna\n",
      "Goldie\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Minh-Thang\n",
      "Luong\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "V\n",
      "Quoc\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Le\n",
      "CoRR, abs/1703.03906\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Long short-term memory-networks for machine reading\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jianpeng\n",
      "Cheng\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Li\n",
      "Dong\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mirella\n",
      "Lapata\n",
      "arXiv:1601.06733\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "arXiv preprint\n",
      "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Learning phrase representations using rnn encoder-decoder for statistical machine translation\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Kyunghyun\n",
      "Cho\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Bart\n",
      "Van Merrienboer\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Caglar\n",
      "Gulcehre\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Fethi\n",
      "Bougares\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Holger\n",
      "Schwenk\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoshua\n",
      "Bengio\n",
      "CoRR, abs/1406.1078\n",
      "\n",
      "\t\t\t\n",
      "2014\n",
      "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Xception: Deep learning with depthwise separable convolutions\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Francois\n",
      "Chollet\n",
      "arXiv:1610.02357\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "arXiv preprint\n",
      "Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Empirical evaluation of gated recurrent neural networks on sequence modeling\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Junyoung\n",
      "Chung\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Çaglar\n",
      "Gülçehre\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Kyunghyun\n",
      "Cho\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoshua\n",
      "Bengio\n",
      "CoRR, abs/1412.3555\n",
      "\n",
      "\t\t\t\n",
      "2014\n",
      "Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Recurrent neural network grammars\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Chris\n",
      "Dyer\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Adhiguna\n",
      "Kuncoro\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Miguel\n",
      "Ballesteros\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Noah\n",
      "A\n",
      "Smith\n",
      "\n",
      "\t\t\n",
      "Proc. of NAACL\n",
      "of NAACL\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jonas\n",
      "Gehring\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Michael\n",
      "Auli\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "David\n",
      "Grangier\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Denis\n",
      "Yarats\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yann\n",
      "N\n",
      "Dauphin\n",
      "arXiv:1705.03122v2\n",
      "Convolutional sequence to sequence learning\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Generating sequences with recurrent neural networks\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Alex\n",
      "Graves\n",
      "arXiv:1308.0850\n",
      "\n",
      "\t\t\t\n",
      "2013\n",
      "arXiv preprint\n",
      "Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Deep residual learning for image recognition\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Kaiming\n",
      "He\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Xiangyu\n",
      "Zhang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Shaoqing\n",
      "Ren\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jian\n",
      "Sun\n",
      "\n",
      "\t\t\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n",
      "the IEEE Conference on Computer Vision and Pattern Recognition\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "None\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Sepp\n",
      "Hochreiter\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoshua\n",
      "Bengio\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Paolo\n",
      "Frasconi\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jürgen\n",
      "Schmidhuber\n",
      "\n",
      "\t\t\t\n",
      "2001\n",
      "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Long short-term memory\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Sepp\n",
      "Hochreiter\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jürgen\n",
      "Schmidhuber\n",
      "\n",
      "\t\t\n",
      "Neural computation\n",
      "\n",
      "\t\t\t\n",
      "9\n",
      "8\n",
      "None\n",
      "1997\n",
      "Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Self-training PCFG grammars with latent annotations across languages\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Zhongqiang\n",
      "Huang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mary\n",
      "Harper\n",
      "\n",
      "\t\t\n",
      "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing\n",
      "the 2009 Conference on Empirical Methods in Natural Language Processing\n",
      "\n",
      "\t\t\t\n",
      "August 2009\n",
      "None\n",
      "Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832-841. ACL, August 2009.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Exploring the limits of language modeling\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Rafal\n",
      "Jozefowicz\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Oriol\n",
      "Vinyals\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mike\n",
      "Schuster\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Noam\n",
      "Shazeer\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yonghui\n",
      "Wu\n",
      "arXiv:1602.02410\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "arXiv preprint\n",
      "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Can active memory replace attention?\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Łukasz\n",
      "Kaiser\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Samy\n",
      "Bengio\n",
      "\n",
      "\t\t\n",
      "Advances in Neural Information Processing Systems, (NIPS)\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Neural GPUs learn algorithms\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Łukasz\n",
      "Kaiser\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ilya\n",
      "Sutskever\n",
      "\n",
      "\t\t\n",
      "International Conference on Learning Representations (ICLR)\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Nal\n",
      "Kalchbrenner\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Lasse\n",
      "Espeholt\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Karen\n",
      "Simonyan\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Aaron\n",
      "Van Den Oord\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Alex\n",
      "Graves\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Koray\n",
      "Kavukcuoglu\n",
      "arXiv:1610.10099v2\n",
      "Neural machine translation in linear time\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Structured attention networks\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoon\n",
      "Kim\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Carl\n",
      "Denton\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Luong\n",
      "Hoang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Alexander\n",
      "M\n",
      "Rush\n",
      "\n",
      "\t\t\n",
      "International Conference on Learning Representations\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Adam: A method for stochastic optimization\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Diederik\n",
      "Kingma\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jimmy\n",
      "Ba\n",
      "\n",
      "\t\t\n",
      "ICLR\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Oleksii\n",
      "Kuchaiev\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Boris\n",
      "Ginsburg\n",
      "arXiv:1703.10722\n",
      "Factorization tricks for LSTM networks\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "A structured self-attentive sentence embedding\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Zhouhan\n",
      "Lin\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Minwei\n",
      "Feng\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Cicero\n",
      "Nogueira Dos Santos\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mo\n",
      "Yu\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Bing\n",
      "Xiang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Bowen\n",
      "Zhou\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yoshua\n",
      "Bengio\n",
      "arXiv:1703.03130\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Multi-task sequence to sequence learning\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Minh-Thang\n",
      "Luong\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Quoc\n",
      "V\n",
      "Le\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ilya\n",
      "Sutskever\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Oriol\n",
      "Vinyals\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Lukasz\n",
      "Kaiser\n",
      "arXiv:1511.06114\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "arXiv preprint\n",
      "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Effective approaches to attentionbased neural machine translation\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Minh-Thang\n",
      "Luong\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Hieu\n",
      "Pham\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Christopher\n",
      "D\n",
      "Manning\n",
      "arXiv:1508.04025\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "arXiv preprint\n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Building a large annotated corpus of english: The penn treebank\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mary\n",
      "Mitchell P Marcus\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ann\n",
      "Marcinkiewicz\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Beatrice\n",
      "Santorini\n",
      "\n",
      "\t\t\n",
      "Computational linguistics\n",
      "\n",
      "\t\t\t\n",
      "19\n",
      "2\n",
      "None\n",
      "1993\n",
      "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Effective self-training for parsing\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "David\n",
      "Mcclosky\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Eugene\n",
      "Charniak\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mark\n",
      "Johnson\n",
      "\n",
      "\t\t\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference\n",
      "the Human Language Technology Conference of the NAACL, Main Conference\n",
      "\n",
      "\t\t\t\n",
      "June 2006\n",
      "None\n",
      "David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159. ACL, June 2006.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "A decomposable attention model\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ankur\n",
      "Parikh\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Oscar\n",
      "Täckström\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Dipanjan\n",
      "Das\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jakob\n",
      "Uszkoreit\n",
      "\n",
      "\t\t\n",
      "Empirical Methods in Natural Language Processing\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Romain\n",
      "Paulus\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Caiming\n",
      "Xiong\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Richard\n",
      "Socher\n",
      "arXiv:1705.04304\n",
      "A deep reinforced model for abstractive summarization\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Learning accurate, compact, and interpretable tree annotation\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Slav\n",
      "Petrov\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Leon\n",
      "Barrett\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Romain\n",
      "Thibaux\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Dan\n",
      "Klein\n",
      "\n",
      "\t\t\n",
      "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL\n",
      "the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL\n",
      "\n",
      "\t\t\t\n",
      "July 2006\n",
      "None\n",
      "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440. ACL, July 2006.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Using the output embedding to improve language models\n",
      "arXiv:1608.05859\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Ofir Press and Lior Wolf\n",
      "arXiv preprint\n",
      "Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Rico\n",
      "Sennrich\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Barry\n",
      "Haddow\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Alexandra\n",
      "Birch\n",
      "arXiv:1508.07909\n",
      "Neural machine translation of rare words with subword units\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "arXiv preprint\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Noam\n",
      "Shazeer\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Azalia\n",
      "Mirhoseini\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Krzysztof\n",
      "Maziarz\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Andy\n",
      "Davis\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Quoc\n",
      "Le\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Geoffrey\n",
      "Hinton\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jeff\n",
      "Dean\n",
      "arXiv:1701.06538\n",
      "\n",
      "\t\t\t\n",
      "2017\n",
      "arXiv preprint\n",
      "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Dropout: a simple way to prevent neural networks from overfitting\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Nitish\n",
      "Srivastava\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Geoffrey\n",
      "E\n",
      "Hinton\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Alex\n",
      "Krizhevsky\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ilya\n",
      "Sutskever\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ruslan\n",
      "Salakhutdinov\n",
      "\n",
      "\t\t\n",
      "Journal of Machine Learning Research\n",
      "\n",
      "\t\t\t\n",
      "15\n",
      "1\n",
      "None\n",
      "2014\n",
      "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958, 2014.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "End-to-end memory networks\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Sainbayar\n",
      "Sukhbaatar\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Arthur\n",
      "Szlam\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jason\n",
      "Weston\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Rob\n",
      "Fergus\n",
      "\n",
      "\t\t\n",
      "Advances in Neural Information Processing Systems\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "C\n",
      "Cortes\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "N\n",
      "D\n",
      "Lawrence\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "D\n",
      "D\n",
      "Lee\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "M\n",
      "Sugiyama\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "R\n",
      "Garnett\n",
      "\n",
      "\t\t\t\n",
      "Curran Associates, Inc\n",
      "2015\n",
      "28\n",
      "None\n",
      "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates, Inc., 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Sequence to sequence learning with neural networks\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ilya\n",
      "Sutskever\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Oriol\n",
      "Vinyals\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Quoc Vv\n",
      "Le\n",
      "\n",
      "\t\t\n",
      "Advances in Neural Information Processing Systems\n",
      "\n",
      "\t\t\t\n",
      "2014\n",
      "None\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Rethinking the inception architecture for computer vision\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Christian\n",
      "Szegedy\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Vincent\n",
      "Vanhoucke\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Sergey\n",
      "Ioffe\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jonathon\n",
      "Shlens\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Zbigniew\n",
      "Wojna\n",
      "CoRR, abs/1512.00567\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Grammar as a foreign language\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Vinyals\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Koo\n",
      "Kaiser\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Petrov\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Sutskever\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Hinton\n",
      "\n",
      "\t\t\n",
      "Advances in Neural Information Processing Systems\n",
      "\n",
      "\t\t\t\n",
      "2015\n",
      "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yonghui\n",
      "Wu\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mike\n",
      "Schuster\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Zhifeng\n",
      "Chen\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "V\n",
      "Quoc\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Mohammad\n",
      "Le\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Wolfgang\n",
      "Norouzi\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Maxim\n",
      "Macherey\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yuan\n",
      "Krikun\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Qin\n",
      "Cao\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Klaus\n",
      "Gao\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Macherey\n",
      "arXiv:1609.08144\n",
      "Google's neural machine translation system: Bridging the gap between human and machine translation\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "arXiv preprint\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Deep recurrent models with fast-forward connections for neural machine translation\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jie\n",
      "Zhou\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Ying\n",
      "Cao\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Xuguang\n",
      "Wang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Peng\n",
      "Li\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Wei\n",
      "Xu\n",
      "CoRR, abs/1606.04199\n",
      "\n",
      "\t\t\t\n",
      "2016\n",
      "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\n",
      "Fast and accurate shift-reduce constituent parsing\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Muhua\n",
      "Zhu\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Yue\n",
      "Zhang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Wenliang\n",
      "Chen\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Min\n",
      "Zhang\n",
      "\n",
      "\t\t\t\n",
      "None\n",
      "Jingbo\n",
      "Zhu\n",
      "\n",
      "\t\t\n",
      "Proceedings of the 51st Annual Meeting of the ACL\n",
      "the 51st Annual Meeting of the ACL\n",
      "\n",
      "\t\t\t\n",
      "August 2013\n",
      "1\n",
      "None\n",
      "Long Papers)\n",
      "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434-443. ACL, August 2013.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Specify the path to your XML file\n",
    "file_path = os.path.join('metas', 'a.pdf.tei.xml')\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Now you can iterate through elements or access specific tags\n",
    "for element in root.iter():\n",
    "    if element.text == None or element.text!= 'None':\n",
    "        print(element.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need\n",
      "Publication Date: 2 Aug 2023\n",
      "Authors: ['Ashish Vaswani', 'Noam Shazeer', 'Google Brain', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan Gomez', 'Łukasz Kaiser', 'Chris Dyer', 'Adhiguna Kuncoro', 'Miguel Ballesteros', 'Noah Smith', 'Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun', 'Sepp Hochreiter', 'Jürgen Schmidhuber', 'Zhongqiang Huang', 'Mary Harper', 'Łukasz Kaiser', 'Samy Bengio', 'Łukasz Kaiser', 'Ilya Sutskever', 'Yoon Kim', 'Carl Denton', 'Luong Hoang', 'Alexander Rush', 'Diederik Kingma', 'Jimmy Ba', 'Mary Mitchell P Marcus', 'Ann Marcinkiewicz', 'Beatrice Santorini', 'David Mcclosky', 'Eugene Charniak', 'Mark Johnson', 'Ankur Parikh', 'Oscar Täckström', 'Dipanjan Das', 'Jakob Uszkoreit', 'Slav Petrov', 'Leon Barrett', 'Romain Thibaux', 'Dan Klein', 'Nitish Srivastava', 'Geoffrey Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov', 'Sainbayar Sukhbaatar', 'Arthur Szlam', 'Jason Weston', 'Rob Fergus', 'Ilya Sutskever', 'Oriol Vinyals', 'Quoc Vv Le', 'None Vinyals', 'Koo Kaiser', 'None Petrov', 'None Sutskever', 'None Hinton', 'Muhua Zhu', 'Yue Zhang', 'Wenliang Chen', 'Min Zhang', 'Jingbo Zhu']\n",
      "Abstract: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load the XML file\n",
    "file_path = os.path.join('metas', 'a.pdf.tei.xml')\n",
    "tree = ET.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define the TEI namespace\n",
    "namespace = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# Function to extract text safely\n",
    "def get_text_or_none(element, tag):\n",
    "    elem = element.find(tag, namespace)\n",
    "    return elem.text if elem is not None else None\n",
    "\n",
    "# Extract title\n",
    "title = get_text_or_none(root, './/tei:titleStmt/tei:title')\n",
    "print(\"Title:\", title)\n",
    "\n",
    "# Extract publication date\n",
    "date = get_text_or_none(root, './/tei:publicationStmt/tei:date')\n",
    "print(\"Publication Date:\", date)\n",
    "\n",
    "# Extract authors\n",
    "authors = []\n",
    "for author in root.findall('.//tei:biblStruct/tei:analytic/tei:author', namespace):\n",
    "    forename = get_text_or_none(author, 'tei:persName/tei:forename')\n",
    "    surname = get_text_or_none(author, 'tei:persName/tei:surname')\n",
    "    authors.append(f\"{forename} {surname}\")\n",
    "print(\"Authors:\", authors)\n",
    "\n",
    "# Extract abstract (if present)\n",
    "abstract = get_text_or_none(root, './/tei:teiHeader/tei:profileDesc/tei:abstract')\n",
    "print(\"Abstract:\", abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def extract_sections_and_text(element, namespaces):\n",
    "    sections = {}\n",
    "    for child in element:\n",
    "        # Check for section-like elements (like div, p, section, etc.)\n",
    "        if child.tag in ['{http://www.tei-c.org/ns/1.0}div', '{http://www.tei-c.org/ns/1.0}p', '{http://www.tei-c.org/ns/1.0}section']:\n",
    "            section_name = child.find('{http://www.tei-c.org/ns/1.0}head', namespaces)  # Assuming section names are in <head> tags\n",
    "            section_name = section_name.text if section_name is not None else \"Unnamed Section\"\n",
    "            sections[section_name] = []\n",
    "\n",
    "            # Collect text from the section\n",
    "            for para in child.xpath('.//tei:p', namespaces):\n",
    "                sections[section_name].append(para.text)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Parse the XML file\n",
    "tree = etree.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define namespaces if needed\n",
    "namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# Extract sections and text\n",
    "all_sections = extract_sections_and_text(root, namespaces)\n",
    "\n",
    "# Output the extracted information\n",
    "for section, texts in all_sections.items():\n",
    "    print(f\"Section: {section}\")\n",
    "    for text in texts:\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from pprint import pprint\n",
    "llm = ChatGroq(model='llama-3.1-70b-versatile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "        \n",
    "    )\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "routing_system_prompt = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains vectors of pdf texts of several research papers. Currently these pdfs are in the vector database: ['Attention is all you need', 'MaGVIT: Mased Generative Video transformer', 'SAN: INDUCING METRIZABILITY OF GAN WITH\n",
    "DISCRIMINATIVE NORMALIZED LINEAR LAYER'].\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", routing_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "# print(\n",
    "#     question_router.invoke(\n",
    "#         {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n",
    "#     )\n",
    "# )\n",
    "print(question_router.invoke({\"question\": 'compare performance of SAN with other generative models?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya-ladawa/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Qdrant client.\n",
      "Collection 'aireas-local' already exists.\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "structured_llm_doc_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "from team_tools import Qretriever\n",
    "\n",
    "retriever = Qretriever\n",
    "\n",
    "# Prompt\n",
    "doc_grader_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", doc_grader_system_prompt),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question = 'compare performance of SAN with other generative models?'\n",
    "retrieval_grader = grade_prompt | structured_llm_doc_grader\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs\n",
    "\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pdf_id': 'SAN.pdf', 'score': 0.7212212}, page_content=\"GAN -Ex∼µ θ [log ς(f (x))] 1 -ς(f (x)) Non-saturating GAN Ex∼µ θ [log ς(1 -f (x))] ς(f (x)) p: A key strength of SAN is that the model can be trained with just two small modifications to existing GAN implementations (see Fig. p: ref: 4 formula: (x) = w ⊤ φ L h φ <L (x) with w φ L ∈ S D-1 p: , where φ l is the parameter for the lth layer. The second is to use the maximization problem defined in Eq. ( p: ref: 15 p: Comparison with other generative models. First, we compare SAN with GAN. In GAN, other than Wasserstein GAN, the optimal discriminator is known to involve a dilemma: it induces a certain dissimilarity but leads to exploding and vanishing gradients p: ref: (Arjovsky et al., 2017; ref: Lin et al., 2021) ref: (Deshpande et al., 2019; ref: Kolouri et al., 2019 ref: 1 ref: (Nguyen et al., 2021; ref: Chen et al., 2022) div: head: EXPERIMENTS p: We perform experiments with synthetic and image datasets to (1) verify our perspective on GANs as presented in Sec. 5 in terms of direction optimality, separability, and injectivity, and (2) show the effectiveness of SAN against GAN. For fair comparisons, we essentially use the same architectures in SAN and GAN. However, we modify the last linear layer of SAN's discriminators (see Sec. 6). div: head: MIXTURE OF GAUSSIANS p: To empirically investigate optimal direction, we conduct experiments on a mixture of Gaussian (MoG). Please refer to Appx. H for empirical verification of the implications of Theorem 5.3 regarding separability and injectivity. We use a two-dimensional sample space X = R 2 . The target MoG on X comprises eight isotropic Gaussians with variances 0.05 2 and means distributed evenly on a circle of radius 1.0. We utilize a 10-dimensional latent space Z to model a generator measure. We compare SAN and GAN with various objectives. As shown in Fig. p: ref: 5 ref: (Srivastava et al., 2017) ref: 6 div: head: IMAGE GENERATION p: We apply the SAN scheme to image generation tasks to show it scales beyond toy experiments. p: DCGAN. We train SANs and GANs with various objective functions on CIFAR10 p: ref:\"),\n",
       " Document(metadata={'pdf_id': 'SAN.pdf', 'score': 0.712113}, page_content='channel=512), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=256), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=128), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=64), BN, ReLU row: cell: Deconv (4×4, stride=1, channel=3), Tanh figure: head: Table 8 : label: 8 figDesc: DCGAN architecture on CelebA. table: row: cell: Discriminator row: cell: Input: x ∈ R 3×128×28 row: cell: 3×3, stride= 1, channel=128, LReLU row: cell: Deconv (4×4, stride=1, channel=512), BN, ReLU cell: SNConv (4 × 4, stride= 2, channel=128), LReLU row: cell: Deconv (4×4, stride=2, channel=256), BN, ReLU cell: SNConv (3×3, stride=1, channel=256), LReLU row: cell: Deconv (4×4, stride=2, channel=128), BN, ReLU cell: SNConv (4×4, stride=2, channel=256), LReLU row: cell: Deconv (4×4, stride=2, channel=64), BN, ReLU cell: SNConv (3×3, stride=1, channel=512), LReLU row: cell: Deconv (4×4, stride=2, channel=32), BN, ReLU cell: SNConv (4×4, stride=2, channel=512), LReLU row: cell: Deconv (4×4, stride=2, channel=3), Tanh cell: SNConv (3×3, stride=1, channel=1024), LReLU row: cell: SNLinear note: p: p: Generator figure: head: Table 9 : label: 9 figDesc: Numerical results for BigGAN figDesc: ref: (Brock et al., 2019) table: row: cell: Method cell: FID (↓) cell: IS (↑) row: cell: BigGAN 17.16±1.34 8.42±0.11 row: cell: BigSAN 14.45±0.58 8.81±0.04 figure: head: Table 10 : label: 10 figDesc: Numerical results for BigGAN figDesc: ref: (Brock et al., 2019) table: row: cell: Method cell: FID (↓) cell: IS (↑) row: cell: BigGAN (baseline) cell: 8.25±0.82 9.05±0.05 row: cell: BigGAN (fine-tuned with SAN) 7.59±0.23 9.04±7.59 row: cell: G SUPPLEMENTARY EXPERIMENTS row: cell: G.1 BIGGAN row: cell: In this section, we present two supplementary experimental results to further support the effective- row: cell: ness of SAN training scheme. figure: head: Table 11 : label: 11 figDesc: Numerical results for StyleGAN-XL figDesc: ref: (Sauer et al., 2022) table: row: cell: Resolution cell: FID (↓) StyleGAN-XL ‡ StyleSAN-XL * StyleGAN-XL StyleSAN-XL * FIDCLIP (↓) row: cell: 256 × 256 cell: (2.19) cell: 1.68 cell:')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SAN outperforms other generative models such as GAN, BigGAN, and StyleGAN-XL '\n",
      " 'in terms of FID (Fréchet Inception Distance) and IS (Inception Score) '\n",
      " 'metrics. Specifically, SAN achieves lower FID scores and higher IS scores '\n",
      " 'compared to its counterparts. For example, BigSAN achieves a FID score of '\n",
      " '14.45±0.58, while BigGAN achieves a score of 17.16±1.34.')\n"
     ]
    }
   ],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        \n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "pprint(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'pdf_id': 'SAN.pdf', 'score': 0.7212212}, page_content=\"GAN -Ex∼µ θ [log ς(f (x))] 1 -ς(f (x)) Non-saturating GAN Ex∼µ θ [log ς(1 -f (x))] ς(f (x)) p: A key strength of SAN is that the model can be trained with just two small modifications to existing GAN implementations (see Fig. p: ref: 4 formula: (x) = w ⊤ φ L h φ <L (x) with w φ L ∈ S D-1 p: , where φ l is the parameter for the lth layer. The second is to use the maximization problem defined in Eq. ( p: ref: 15 p: Comparison with other generative models. First, we compare SAN with GAN. In GAN, other than Wasserstein GAN, the optimal discriminator is known to involve a dilemma: it induces a certain dissimilarity but leads to exploding and vanishing gradients p: ref: (Arjovsky et al., 2017; ref: Lin et al., 2021) ref: (Deshpande et al., 2019; ref: Kolouri et al., 2019 ref: 1 ref: (Nguyen et al., 2021; ref: Chen et al., 2022) div: head: EXPERIMENTS p: We perform experiments with synthetic and image datasets to (1) verify our perspective on GANs as presented in Sec. 5 in terms of direction optimality, separability, and injectivity, and (2) show the effectiveness of SAN against GAN. For fair comparisons, we essentially use the same architectures in SAN and GAN. However, we modify the last linear layer of SAN's discriminators (see Sec. 6). div: head: MIXTURE OF GAUSSIANS p: To empirically investigate optimal direction, we conduct experiments on a mixture of Gaussian (MoG). Please refer to Appx. H for empirical verification of the implications of Theorem 5.3 regarding separability and injectivity. We use a two-dimensional sample space X = R 2 . The target MoG on X comprises eight isotropic Gaussians with variances 0.05 2 and means distributed evenly on a circle of radius 1.0. We utilize a 10-dimensional latent space Z to model a generator measure. We compare SAN and GAN with various objectives. As shown in Fig. p: ref: 5 ref: (Srivastava et al., 2017) ref: 6 div: head: IMAGE GENERATION p: We apply the SAN scheme to image generation tasks to show it scales beyond toy experiments. p: DCGAN. We train SANs and GANs with various objective functions on CIFAR10 p: ref:\"),\n",
       "  Document(metadata={'pdf_id': 'SAN.pdf', 'score': 0.712113}, page_content='channel=512), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=256), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=128), BN, ReLU row: cell: Deconv (4×4, stride=2, channel=64), BN, ReLU row: cell: Deconv (4×4, stride=1, channel=3), Tanh figure: head: Table 8 : label: 8 figDesc: DCGAN architecture on CelebA. table: row: cell: Discriminator row: cell: Input: x ∈ R 3×128×28 row: cell: 3×3, stride= 1, channel=128, LReLU row: cell: Deconv (4×4, stride=1, channel=512), BN, ReLU cell: SNConv (4 × 4, stride= 2, channel=128), LReLU row: cell: Deconv (4×4, stride=2, channel=256), BN, ReLU cell: SNConv (3×3, stride=1, channel=256), LReLU row: cell: Deconv (4×4, stride=2, channel=128), BN, ReLU cell: SNConv (4×4, stride=2, channel=256), LReLU row: cell: Deconv (4×4, stride=2, channel=64), BN, ReLU cell: SNConv (3×3, stride=1, channel=512), LReLU row: cell: Deconv (4×4, stride=2, channel=32), BN, ReLU cell: SNConv (4×4, stride=2, channel=512), LReLU row: cell: Deconv (4×4, stride=2, channel=3), Tanh cell: SNConv (3×3, stride=1, channel=1024), LReLU row: cell: SNLinear note: p: p: Generator figure: head: Table 9 : label: 9 figDesc: Numerical results for BigGAN figDesc: ref: (Brock et al., 2019) table: row: cell: Method cell: FID (↓) cell: IS (↑) row: cell: BigGAN 17.16±1.34 8.42±0.11 row: cell: BigSAN 14.45±0.58 8.81±0.04 figure: head: Table 10 : label: 10 figDesc: Numerical results for BigGAN figDesc: ref: (Brock et al., 2019) table: row: cell: Method cell: FID (↓) cell: IS (↑) row: cell: BigGAN (baseline) cell: 8.25±0.82 9.05±0.05 row: cell: BigGAN (fine-tuned with SAN) 7.59±0.23 9.04±7.59 row: cell: G SUPPLEMENTARY EXPERIMENTS row: cell: G.1 BIGGAN row: cell: In this section, we present two supplementary experimental results to further support the effective- row: cell: ness of SAN training scheme. figure: head: Table 11 : label: 11 figDesc: Numerical results for StyleGAN-XL figDesc: ref: (Sauer et al., 2022) table: row: cell: Resolution cell: FID (↓) StyleGAN-XL ‡ StyleSAN-XL * StyleGAN-XL StyleSAN-XL * FIDCLIP (↓) row: cell: 256 × 256 cell: (2.19) cell: 1.68 cell:')],\n",
       " 'question': 'compare performance of SAN with other generative models?',\n",
       " 'answer': 'SAN outperforms GAN in experiments on synthetic and image datasets. For image generation tasks, SAN achieves better results than GAN, with lower FID scores and higher IS scores. Specifically, SAN variants (e.g., BigSAN, StyleSAN-XL) outperform their GAN counterparts (e.g., BigGAN, StyleGAN-XL).'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "rag_chain_with_source.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "structured_llm_hallucinations_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "hallucination_grader_system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hallucination_grader_system_prompt),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_hallucinations_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "structured_llm_answer_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "answer_grader_system_prompt = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", answer_grader_system_prompt),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_answer_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s an improved version of the question:\\n\\n\"What are the different methods for transferring video content, including analog and digital formats, from one device or medium to another?\"\\n\\nThis revised question provides more context and specificity, which can help improve retrieval based on its semantic intent. \\n\\nAlternatively, you could also ask:\\n\\n* \"What video transfer methods are available for converting, copying, or moving video content?\"\\n* \"What are the various techniques used for video migration, duplication, or conversion between different devices or formats?\"\\n* \"How can video content be transferred from one device or medium to another, and what are the different techniques involved?\"'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "rewriter_system_prompt = \"\"\"Rewrite the question to optimize it for retrieval based on its semantic intent.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewriter_system_prompt),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_tools import tavily_search_tool\n",
    "\n",
    "web_search_tool = tavily_search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": [web_results], \"question\": question}\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"in attention is all you need paper, what is positional encoding?\"\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the metadata of paper m.pdf?',\n",
      " 'Get the title from the retrieved metadata.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pdf_id': 'SAN.pdf',\n",
       "  'score': 0.55051386,\n",
       "  'page_content': '(ICLR), 2023.\\nNicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pfister. Sliced and radon Wasserstein\\nbarycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015.\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\\nimage synthesis. In Proc. International Conference on Learning Representation (ICLR), 2019.\\nRobin Chan, Sarina Penquitt, and Hanno Gottschalk. Lu-net: Invertible neural networks based on\\nmatrix factorization. arXiv preprint arXiv:2302.10524, 2023.\\nTatjana Chavdarova, Gauthier Gidel, Franc¸ois Fleuret, and Simon Lacoste-Julien. Reducing noise\\nin gan training with variance reduced extragradient. In Proc. Advances in Neural Information\\nProcessing Systems (NeurIPS), volume 32, 2019.\\nTatjana Chavdarova, Matteo Pagliardini, Sebastian U Stich, Franc¸ois Fleuret, and Martin Jaggi.\\nTaming gans with lookahead-minmax. In Proc. International Conference on Learning Represen-\\ntation (ICLR), 2022.\\nXiongjie Chen, Yongxin Yang, and Yunpeng Li. Augmented sliced wasserstein distances. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2022.\\nCasey Chu, Kentaro Minami, and Kenji Fukumizu. Smoothness and stability in GANs. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2020.\\nIshan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen\\nZhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for\\nGANs. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\npp. 10648–10656, 2019.\\n10\\nPublished as a conference paper at ICLR 2024\\nChris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In Proc. Inter-\\nnational Conference on Learning Representation (ICLR), 2019.\\nJiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein\\ngradient flow. In Proc. International Conference on Machine Learning (ICML), volume 162, pp.\\n6185–6215, 2022.'},\n",
       " {'pdf_id': 'SAN.pdf',\n",
       "  'score': 0.54721797,\n",
       "  'page_content': 'Representation (ICLR), 2016.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natural language supervision. In Proc. In-\\nternational Conference on Machine Learning (ICML), pp. 8748–8763, 2021.\\nLillian J Ratliff, Samuel A Burden, and S Shankar Sastry. Characterization and computation of local\\nnash equilibria in continuous games. In Annual Allerton Conference on Communication, Control,\\nand Computing (Allerton), pp. 917–924. IEEE, 2013.\\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\\nProc. International Conference on Machine Learning (ICML), pp. 1530–1538, 2015.\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\\nrecognition challenge. International journal of computer vision, 115:211–252, 2015.\\n13\\nPublished as a conference paper at ICLR 2024\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\nImproved techniques for training GANs. In Proc. Advances in Neural Information Processing\\nSystems (NeurIPS), volume 29, 2016.\\nMaziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robust-\\nness of training gans with regularized optimal transport. In Proc. International Conference on\\nMachine Learning (ICML), volume 31, 2018.\\nAxel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas Geiger. Projected gans converge faster. In\\nProc. Advances in Neural Information Processing Systems (NeurIPS), pp. 17480–17492, 2021.\\nAxel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse\\ndatasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 1–10, 2022.\\nAxel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the\\npower of gans for fast large-scale text-to-image synthesis. In Proc. International Conference on'},\n",
       " {'pdf_id': 'SAN.pdf',\n",
       "  'score': 0.55282915,\n",
       "  'page_content': 'Representation (ICLR), 2016.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natural language supervision. In Proc. In-\\nternational Conference on Machine Learning (ICML), pp. 8748–8763, 2021.\\nLillian J Ratliff, Samuel A Burden, and S Shankar Sastry. Characterization and computation of local\\nnash equilibria in continuous games. In Annual Allerton Conference on Communication, Control,\\nand Computing (Allerton), pp. 917–924. IEEE, 2013.\\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\\nProc. International Conference on Machine Learning (ICML), pp. 1530–1538, 2015.\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\\nrecognition challenge. International journal of computer vision, 115:211–252, 2015.\\n13\\nPublished as a conference paper at ICLR 2024\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\nImproved techniques for training GANs. In Proc. Advances in Neural Information Processing\\nSystems (NeurIPS), volume 29, 2016.\\nMaziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robust-\\nness of training gans with regularized optimal transport. In Proc. International Conference on\\nMachine Learning (ICML), volume 31, 2018.\\nAxel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas Geiger. Projected gans converge faster. In\\nProc. Advances in Neural Information Processing Systems (NeurIPS), pp. 17480–17492, 2021.\\nAxel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse\\ndatasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 1–10, 2022.\\nAxel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the\\npower of gans for fast large-scale text-to-image synthesis. In Proc. International Conference on'},\n",
       " {'pdf_id': 'SAN.pdf',\n",
       "  'score': 0.55001175,\n",
       "  'page_content': '(ICLR), 2023.\\nNicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pfister. Sliced and radon Wasserstein\\nbarycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015.\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\\nimage synthesis. In Proc. International Conference on Learning Representation (ICLR), 2019.\\nRobin Chan, Sarina Penquitt, and Hanno Gottschalk. Lu-net: Invertible neural networks based on\\nmatrix factorization. arXiv preprint arXiv:2302.10524, 2023.\\nTatjana Chavdarova, Gauthier Gidel, Franc¸ois Fleuret, and Simon Lacoste-Julien. Reducing noise\\nin gan training with variance reduced extragradient. In Proc. Advances in Neural Information\\nProcessing Systems (NeurIPS), volume 32, 2019.\\nTatjana Chavdarova, Matteo Pagliardini, Sebastian U Stich, Franc¸ois Fleuret, and Martin Jaggi.\\nTaming gans with lookahead-minmax. In Proc. International Conference on Learning Represen-\\ntation (ICLR), 2022.\\nXiongjie Chen, Yongxin Yang, and Yunpeng Li. Augmented sliced wasserstein distances. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2022.\\nCasey Chu, Kentaro Minami, and Kenji Fukumizu. Smoothness and stability in GANs. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2020.\\nIshan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen\\nZhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for\\nGANs. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\npp. 10648–10656, 2019.\\n10\\nPublished as a conference paper at ICLR 2024\\nChris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In Proc. Inter-\\nnational Conference on Learning Representation (ICLR), 2019.\\nJiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein\\ngradient flow. In Proc. International Conference on Machine Learning (ICML), volume 162, pp.\\n6185–6215, 2022.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from team_tools import Qretriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk\n",
    "def parse(ai_message: AIMessage) -> List[str]:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    # Split the input string by newline, strip whitespace and single quotes, and return a list of queries\n",
    "    return [line for line in ai_message.content.split('\\n')]\n",
    "\n",
    "\n",
    "# Define the query prompt\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Break down complex queries into simpler ones. Please adhere to the context and topic of the original question. Rephrase the simpler queries to make them efficient for querying a vector database.\n",
    "    Please try to only generate the most minimum/least number of queries required to answer the question. Do not add repetitive or irrelevant questions.\n",
    "ONLY PROVIDE THE OUTPUT. List the questions seperated by newlines.\n",
    "\n",
    "Example input: What is attention mechanism? How can i explain it to a child?\n",
    "Output:\n",
    "Explain attention mechanism in deep learning.\n",
    "Give an example to explain attention mechanism to 5 year old.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the LLM chain\n",
    "llm_chain = QUERY_PROMPT | llm | parse\n",
    "\n",
    "question = 'what is the title of paper m.pdf'\n",
    "\n",
    "pprint(llm_chain.invoke(question))\n",
    "\n",
    "# Initialize the MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=Qretriever, llm_chain=llm_chain\n",
    ")\n",
    "\n",
    "# Run the retriever\n",
    "results = retriever.invoke(question)\n",
    "\n",
    "\n",
    "# Assuming `documents` is a list of Document objects\n",
    "refines = [{ 'pdf_id': doc.metadata['pdf_id'], 'score': doc.metadata['score'], 'page_content': doc.page_content } for doc in results]\n",
    "refines\n",
    "# print(refines[::-1])\n",
    "# print('\\n')\n",
    "\n",
    "# llm.invoke(f'{question}\\n{refines[::-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# from typing import Optional\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# # Define the output schema for parsing\n",
    "# class DocumentDetectionOutput(BaseModel):\n",
    "#     pdf_name: Optional[str] = Field(None, description=\"The name of the PDF file mentioned in the question, if any.\")\n",
    "\n",
    "# # Define the parsing function to interpret the output\n",
    "# def parse_document_name(output: str) -> DocumentDetectionOutput:\n",
    "#     \"\"\"\n",
    "#     Parses the LLM output to detect if a specific PDF file name was mentioned.\n",
    "#     Expected format: 'The pdf_name is: <file_name>'\n",
    "#     \"\"\"\n",
    "#     output_lines = output.strip().splitlines()\n",
    "#     for line in output_lines:\n",
    "#         if \"pdf_name is:\" in line:\n",
    "#             pdf_name = line.split(\"pdf_name is:\")[-1].strip()\n",
    "#             return DocumentDetectionOutput(pdf_name=pdf_name if pdf_name else None)\n",
    "#     return DocumentDetectionOutput(pdf_name=None)\n",
    "\n",
    "# # Prompt Template to detect document mentions\n",
    "# DOC_DETECTION_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"Identify if the question pertains to any specific document by name.\n",
    "#     If a PDF is mentioned, output the name in the format: 'The pdf_name is: <file_name>'.\n",
    "#     If no specific document is mentioned, respond with: 'No specific document mentioned.'\n",
    "    \n",
    "#     Question: {question}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# # Initialize LLM Chain\n",
    "# llm_chain = DOC_DETECTION_PROMPT | llm \n",
    "\n",
    "# # Example function to use the chain\n",
    "# def detect_pdf_name(question: str):\n",
    "#     # Run the LLM chain\n",
    "#     output = llm_chain.invoke(question)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# detect_pdf_name('what is the title of pdf \"a\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata and self query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya-ladawa/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Qdrant client.\n",
      "Collection 'aireas-cloud' already exists.\n",
      "Index on 'pdf_id' field already exists.\n",
      "[Document(metadata={'pdf_id': 'a.pdf', '_id': 'e104188a-f98b-4908-b74c-52c1e15d3759', '_collection_name': 'aireas-cloud'}, page_content=''), Document(metadata={'pdf_id': 'a.pdf', '_id': '70ab1791-77cf-42a5-81fe-36e7336b3aab', '_collection_name': 'aireas-cloud'}, page_content=''), Document(metadata={'pdf_id': 'a.pdf', '_id': '1af2ec7c-f855-4e36-92e1-e516bca933f6', '_collection_name': 'aireas-cloud'}, page_content=''), Document(metadata={'pdf_id': 'a.pdf', '_id': '241ceb9f-8a5c-4a58-b082-aa6c24ae4910', '_collection_name': 'aireas-cloud'}, page_content='')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_cloud_ops import URL, EMBEDDING_MODEL, QDRANT_API_KEY\n",
    "\n",
    "# Qdrant Docker is running on localhost, port 6333\n",
    "url = URL\n",
    "\n",
    "# Initialize QdrantVectorStore instance\n",
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    collection_name=\"aireas-cloud\",\n",
    "    embedding=EMBEDDING_MODEL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    content_payload_key=\"text\",  # Main text field\n",
    "    metadata_payload_key=\"metadata\"  # Metadata field\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pdf_id': 'san.pdf', '_id': '549f21e5-98f8-4d52-9923-18e2be4a7eaf', '_collection_name': 'aireas-cloud'}, page_content='Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the\\npower of gans for fast large-scale text-to-image synthesis. In Proc. International Conference on\\nMachine Learning (ICML), 2023.\\nSamarth Sinha, Anirudh Goyal, Colin Raffel, and Augustus Odena. Top-k training of GANs: Im-\\nproving generators by making critics less critical. In Proc. Advances in Neural Information Pro-\\ncessing Systems (NeurIPS), 2020.\\nJiaming Song and Stefano Ermon. Bridging the gap between f-gans and wasserstein gans. In Proc.\\nInternational Conference on Machine Learning (ICML), pp. 9078–9087, 2020.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2020.\\nYang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with\\nmasked convolutions. In Proc. Advances in Neural Information Processing Systems (NeurIPS),\\nvolume 32, 2019.\\nAkash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. VEEGAN:\\nReducing mode collapse in gans using implicit variational learning. In Proc. Advances in Neural\\nInformation Processing Systems (NeurIPS), volume 30, 2017.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\\nthe inception architecture for computer vision. In Proc. IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), pp. 2818–2826, 2016.\\nEsteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.\\nCommunications on Pure and Applied Mathematics, 66(2):145–164, 2013.\\nEsteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.\\nCommunications in Mathematical Sciences, 8(1):217–233, 2010.\\nYuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Ue-\\nsaka, Naoki Murata, Takahashi Shusuke, Toshiyuki Kumakura, and Yuki Mitsufuji. SQ-VAE:\\nVariational Bayes on discrete representation with self-annealed stochastic quantization. In Proc.'),\n",
       " Document(metadata={'pdf_id': 'san.pdf', '_id': '16c40886-971b-4114-b37b-593c3ae37f1f', '_collection_name': 'aireas-cloud'}, page_content='Representation (ICLR), 2016.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natural language supervision. In Proc. In-\\nternational Conference on Machine Learning (ICML), pp. 8748–8763, 2021.\\nLillian J Ratliff, Samuel A Burden, and S Shankar Sastry. Characterization and computation of local\\nnash equilibria in continuous games. In Annual Allerton Conference on Communication, Control,\\nand Computing (Allerton), pp. 917–924. IEEE, 2013.\\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\\nProc. International Conference on Machine Learning (ICML), pp. 1530–1538, 2015.\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\\nrecognition challenge. International journal of computer vision, 115:211–252, 2015.\\n13\\nPublished as a conference paper at ICLR 2024\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\nImproved techniques for training GANs. In Proc. Advances in Neural Information Processing\\nSystems (NeurIPS), volume 29, 2016.\\nMaziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robust-\\nness of training gans with regularized optimal transport. In Proc. International Conference on\\nMachine Learning (ICML), volume 31, 2018.\\nAxel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas Geiger. Projected gans converge faster. In\\nProc. Advances in Neural Information Processing Systems (NeurIPS), pp. 17480–17492, 2021.\\nAxel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse\\ndatasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 1–10, 2022.\\nAxel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the\\npower of gans for fast large-scale text-to-image synthesis. In Proc. International Conference on'),\n",
       " Document(metadata={'pdf_id': 'san.pdf', '_id': '272cddbd-1c53-4059-82b8-a46071745b51', '_collection_name': 'aireas-cloud'}, page_content='(ICLR), 2023.\\nNicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pfister. Sliced and radon Wasserstein\\nbarycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015.\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\\nimage synthesis. In Proc. International Conference on Learning Representation (ICLR), 2019.\\nRobin Chan, Sarina Penquitt, and Hanno Gottschalk. Lu-net: Invertible neural networks based on\\nmatrix factorization. arXiv preprint arXiv:2302.10524, 2023.\\nTatjana Chavdarova, Gauthier Gidel, Franc¸ois Fleuret, and Simon Lacoste-Julien. Reducing noise\\nin gan training with variance reduced extragradient. In Proc. Advances in Neural Information\\nProcessing Systems (NeurIPS), volume 32, 2019.\\nTatjana Chavdarova, Matteo Pagliardini, Sebastian U Stich, Franc¸ois Fleuret, and Martin Jaggi.\\nTaming gans with lookahead-minmax. In Proc. International Conference on Learning Represen-\\ntation (ICLR), 2022.\\nXiongjie Chen, Yongxin Yang, and Yunpeng Li. Augmented sliced wasserstein distances. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2022.\\nCasey Chu, Kentaro Minami, and Kenji Fukumizu. Smoothness and stability in GANs. In Proc.\\nInternational Conference on Learning Representation (ICLR), 2020.\\nIshan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen\\nZhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for\\nGANs. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\npp. 10648–10656, 2019.\\n10\\nPublished as a conference paper at ICLR 2024\\nChris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In Proc. Inter-\\nnational Conference on Learning Representation (ICLR), 2019.\\nJiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein\\ngradient flow. In Proc. International Conference on Machine Learning (ICML), volume 162, pp.\\n6185–6215, 2022.'),\n",
       " Document(metadata={'pdf_id': 'san.pdf', '_id': 'a1e29530-cbae-499a-9c4b-eb00943cc30d', '_collection_name': 'aireas-cloud'}, page_content='saka, Naoki Murata, Takahashi Shusuke, Toshiyuki Kumakura, and Yuki Mitsufuji. SQ-VAE:\\nVariational Bayes on discrete representation with self-annealed stochastic quantization. In Proc.\\nInternational Conference on Machine Learning (ICML), 2022.\\nMingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural net-\\nworks. In Proc. International Conference on Machine Learning (ICML), pp. 6105–6114, 2019.\\nD´avid Terj´ek. Adversarial lipschitz regularization. In Proc. International Conference on Learning\\nRepresentation (ICLR), 2020.\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\\nHerve Jegou. Training data-efficient image transformers & distillation through attention. In Proc.\\nInternational Conference on Machine Learning (ICML), pp. 10347–10357, 2021.\\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion\\nand content for video generation. In Proc. IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pp. 1526–1535, 2018.\\n14\\nPublished as a conference paper at ICLR 2024\\nC´edric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\\nKun Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Understanding and stabilizing GANs’ training\\ndynamics with control theory. In Proc. International Conference on Machine Learning (ICML),\\npp. 10566–10575, 2020.\\nYasin Yaz, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, Vijay Chan-\\ndrasekhar, et al. The unusual effectiveness of averaging in gan training. In Proc. International\\nConference on Learning Representation (ICLR), 2019.\\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.\\nSelf-attention generative\\nadversarial networks. In Proc. International Conference on Machine Learning (ICML), pp. 7354–\\n7363, 2019.\\nHan Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee.\\nConsistency regularization for\\ngenerative adversarial networks. In Proc. International Conference on Learning Representation\\n(ICLR), 2020.\\nShengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing learning and inference in')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"pdf_id\",\n",
    "        description=\"The name of the research paper which is in .pdf filetype.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = 'texts of several research papers, academic papers, and scholarly articles.'\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, qdrant, document_content_description, metadata_field_info, verbose=True\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "# chain = Qretriever | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "retriever.invoke(\"what is abstract of san.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qdrant count and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "from team_tools import EMBEDDING_MODEL\n",
    "\n",
    "# Initialize your Qdrant client\n",
    "q_client = QdrantClient(\"http://localhost:6333\")  # Adjust the URL as needed\n",
    "\n",
    "def count_points(qclient: QdrantClient, value_: str, coll_name: str = 'aireas-local', key_: str = 'pdf_id') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of points associated with a specific pdf_id.\n",
    "\n",
    "    Args:\n",
    "        qclient (QdrantClient): The Qdrant client instance.\n",
    "        pdf_id (str): The PDF identifier to count points for.\n",
    "        coll_name (str): The name of the collection. Default is 'aireas-local'.\n",
    "        key (str): The key to match against. Default is 'pdf_id'.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of points associated with the specified pdf_id.\n",
    "    \"\"\"\n",
    "    count_response = qclient.count(\n",
    "        collection_name=coll_name,\n",
    "        count_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=key_, match=MatchValue(value=value_)),\n",
    "            ]\n",
    "        ),\n",
    "        exact=True,\n",
    "    )\n",
    "    return int(count_response.count)\n",
    "\n",
    "def query_points(query_text_: str, key_ : str, qclient: QdrantClient, value_: str, limit: int = 3, coll_name: str = 'aireas-local') -> list:\n",
    "    \"\"\"\n",
    "    Query points from the Qdrant collection based on a pdf_id and a query string.\n",
    "\n",
    "    Args:\n",
    "        qclient (QdrantClient): The Qdrant client instance.\n",
    "        pdf_id (str): The PDF identifier to filter points.\n",
    "        limit (int): The maximum number of points to return. Default is 3.\n",
    "        query_text (str): The text to embed for querying. Default is 'references'.\n",
    "        coll_name (str): The name of the collection. Default is 'aireas-local'.\n",
    "\n",
    "    Returns:\n",
    "        list: The results from the query, containing points matching the criteria.\n",
    "    \"\"\"\n",
    "    # Generate the query embedding\n",
    "    query_embedding = EMBEDDING_MODEL.embed_query(query_text_)\n",
    "    prefetch_limit = count_points(qclient, value_, coll_name)\n",
    "    \n",
    "    # Execute the query\n",
    "    results = qclient.query_points(\n",
    "        collection_name=coll_name,\n",
    "        query=query_embedding,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                filter=models.Filter(\n",
    "                    must=models.FieldCondition(\n",
    "                        key=key_,\n",
    "                        match=models.MatchValue(value=value_),\n",
    "                    ),\n",
    "                ),\n",
    "                limit=prefetch_limit  # Prefetch limit, adjust as needed\n",
    "            ),\n",
    "        ],\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# # Usage example\n",
    "# pdf_id = 'a.pdf'  # Replace with your actual pdf_id\n",
    "# points_count = count_points(q_client, pdf_id)  # Get the count of points for the specified pdf_id\n",
    "# print(f\"Number of points associated with '{pdf_id}': {points_count}\")\n",
    "\n",
    "# # Query points for the specified pdf_id\n",
    "# query_results = query_points(q_client, pdf_id)\n",
    "# query_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='80dd3aa3-35f6-4853-953d-d9ac7bae6c09', version=4, score=0.502875, payload={'pdf_id': 'a.pdf', 'text': 'corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='3c909532-8d98-4b5f-8ca4-6a85452e3675', version=4, score=0.4819668, payload={'pdf_id': 'a.pdf', 'text': 'arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='ea18108f-aa42-4467-8e6f-a1b76bdc85de', version=4, score=0.4719031, payload={'pdf_id': 'a.pdf', 'text': '9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# import os\n",
    "# # Open the PDF file\n",
    "# pdf_document = fitz.open(os.path.join(\"static/files/\", 'm.pdf'))\n",
    "\n",
    "# # Initialize a variable to hold all text\n",
    "# all_text = \"\"\n",
    "\n",
    "# # Iterate through each page and extract text\n",
    "# for page_num in range(len(pdf_document)):\n",
    "#     page = pdf_document.load_page(page_num)  # Load the page\n",
    "#     all_text += page.get_text()  # Extract text from the page\n",
    "\n",
    "# # Close the PDF document\n",
    "# pdf_document.close()\n",
    "\n",
    "# # Print all extracted text\n",
    "# print(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aditya-ladawa/Aditya/z_projects/aireas/api'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parallel runnable pass through chain to get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from team_tools import Qretriever\n",
    "\n",
    "# retriever = Qretriever\n",
    "\n",
    "# trimmer = trim_messages(\n",
    "#     max_tokens=120000,\n",
    "#     strategy=\"last\",\n",
    "#     token_counter=llm,\n",
    "#     include_system=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# # Post-processing\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# from langchain_core.runnables import RunnableParallel\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# rag_chain_from_docs = (\n",
    "#     RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# rag_chain_with_source = RunnableParallel(\n",
    "#     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "# ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "# rag_chain_with_source.invoke(\"What is attention\")\n",
    "\n",
    "# response = rag_chain_with_source.invoke(\"What is attention, and how is it realted to SAN pdf\")\n",
    "\n",
    "# # Access the answer and sources\n",
    "# answer = response['answer']\n",
    "# sources = [doc.metadata['pdf_id'] for doc in response['context']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant for fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Qdrant client\n",
    "# @app.on_event(\"startup\")\n",
    "# async def startup_event():\n",
    "#     client = connect_to_qdrant()\n",
    "#     app.state.qdrant_client = client\n",
    "\n",
    "# @app.on_event(\"shutdown\")\n",
    "# async def shutdown_event():\n",
    "#     client = app.state.qdrant_client\n",
    "#     if client:\n",
    "#         print(\"Shutting down Qdrant client\")\n",
    "#         client.close()\n",
    "\n",
    "# # Qdrant client\n",
    "# def get_qdrant_client():\n",
    "#     return app.state.qdrant_client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are an assistant that decomposes complex questions into simpler, isolated sub-questions. Your task is to break down the given question into the smallest possible set of essential sub-problems or sub-questions, with a limit of 3 sub-questions. Each sub-question should be answerable independently and necessary to address the original question.\n",
    "If the question demands it then only you need to break down the question into simpler sub-questions.\n",
    "Respond only with the sub-questions, separated by newline.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the title of the file named a.pdf?',\n",
       " 'What is the content of the file a.pdf?',\n",
       " 'Is there an abstract or summary within the content of the file a.pdf?']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model='llama-3.1-70b-versatile')\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"what is the title of a.pdf and also explain me the abstract of this file\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the title of the PDF file 'a.pdf'?\",\n",
       " \"What is the content of the PDF file 'a.pdf'?\",\n",
       " \"What is the abstract or summary of the content of the PDF file 'a.pdf'?\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The title of the PDF file 'a.pdf' cannot be determined from the context provided.\",\n",
       " \"The PDF file 'a.pdf' contains text about a neural network model, specifically a transformer model, including its architecture, components such as encoder and decoder, attention mechanisms, and positional encoding. The text also includes visualizations of attention mechanisms and examples of the model's behavior.\",\n",
       " 'The content cannot be summarized into an abstract as the provided context appears to be a list of references or citations from various research papers, without any narrative or descriptive text to summarize.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_answer = final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search_agent_test_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_agent_prompt = '''\n",
    "# You are a helpful assistant which given a query will use appropriate tool.\n",
    "# Tools available to you:\n",
    "# 1. tavily_search_tool - Useful when you want to search something on internet.\n",
    "# 2. web_scraper_tool - useful when you need to scrape the contents of website when given website link/s.\n",
    "# 3. arxiv_search_tool - useful when you need to get citations or information on a certain research paper.\n",
    "\n",
    "# '''\n",
    "# search_agent = create_react_agent(llm, tools=[tavily_search_tool, web_scraper_tool, arxiv_search_tool], state_modifier=search_agent_prompt)\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"what is attention mechanism in transformer? what are the recent updates?\")]}\n",
    "\n",
    "# for s in search_agent.stream(inputs, stream_mode=\"values\"):\n",
    "#   message = s[\"messages\"][-1]\n",
    "#   if isinstance(message, tuple):\n",
    "#       print(message)\n",
    "#   else:\n",
    "#       message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# research team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research team with each tool as react agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class SearchTeamState(TypedDict):\n",
    "#     messages: Annotated[List[BaseMessage], operator.add]\n",
    "#     team_members: List[str]\n",
    "#     next: str\n",
    "\n",
    "# search_agent = create_react_agent(llm, tools=[tavily_search_tool, web_scraper_tool, arxiv_search_tool])\n",
    "# search_node = functools.partial(agent_node, agent=search_agent, name=\"Search\")\n",
    "\n",
    "# web_search_agent = create_react_agent(llm, tools=[web_scraper_tool])\n",
    "# web_search_node = functools.partial(agent_node, agent=web_search_agent, name=\"WebScraper\")\n",
    "\n",
    "# arxiv_search_agent = create_react_agent(llm, tools=[arxiv_search_tool])\n",
    "# arxiv_search_node = functools.partial(agent_node, agent=arxiv_search_agent, name=\"ArxivSearch\")\n",
    "\n",
    "# search_supervisor_agent = create_team_supervisor(\n",
    "#     llm,\n",
    "#     \"\"\"\n",
    "#         Supervisor Instructions for Task Management\n",
    "\n",
    "#         You are assigned as the supervisor overseeing a conversation involving the following worker agents:\n",
    "#         - **TavilySearch**\n",
    "#         - **WebScraper**\n",
    "#         - **ArxivSearch**\n",
    "\n",
    "#         ### Objective\n",
    "#         Based on the given user request, decide which worker should execute the next task in the workflow. \n",
    "#         Each worker will perform its assigned task and return its results along with a status update.\n",
    "\n",
    "#         ### Guidelines\n",
    "#         - **Efficiency**: Minimize search steps to save the user's time.\n",
    "#         - **Completion**: Once all necessary tasks are complete, respond with FINISH.\n",
    "# \"\"\",\n",
    "#     [\"TavilySearch\", \"WebScraper\", \"ArxivSearch\"],\n",
    "# )\n",
    "\n",
    "# search_graph = StateGraph(SearchTeamState)\n",
    "\n",
    "# search_graph.add_node('TavilySearch', tavily_search_node)\n",
    "# search_graph.add_node('WebScraper', web_search_node)\n",
    "# search_graph.add_node('ArxivSearch', arxiv_search_node)\n",
    "# search_graph.add_node('SearchSupervisor', search_supervisor_agent)\n",
    "\n",
    "# search_graph.add_edge('TavilySearch', 'SearchSupervisor')\n",
    "# search_graph.add_edge('WebScraper', 'SearchSupervisor')\n",
    "# search_graph.add_edge('ArxivSearch', 'SearchSupervisor')\n",
    "\n",
    "# search_graph.add_conditional_edges(\n",
    "#     'SearchSupervisor',\n",
    "#     lambda x: x['next'],\n",
    "#     {\"TavilySearch\": \"TavilySearch\", \"WebScraper\": \"WebScraper\", \"ArxivSearch\": \"ArxivSearch\", \"FINISH\": END}\n",
    "# )\n",
    "\n",
    "# search_graph.add_edge(START, 'SearchSupervisor')\n",
    "\n",
    "# search_graph_compile = search_graph.compile()\n",
    "# search_chain = enter_chain | search_graph_compile \n",
    "\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(search_graph_compile.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# for s in search_chain.stream(\n",
    "#     \"what is a transformer, and tell me the latest updates regardding the vdieo transformer by open ai\", {\"recursion_limit\": 100}\n",
    "# ):\n",
    "#     if \"__end__\" not in s:\n",
    "#         print(s)\n",
    "#         print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## research team with react search agent with all search tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResearchTeamState(TypedDict):\n",
    "#     messages: Annotated[List[BaseMessage], operator.add]\n",
    "#     team_members: List[str]\n",
    "#     next: str\n",
    "\n",
    "# rag_agent = create_react_agent(llm, tools=[qdrant_retriever_tool])\n",
    "# rag_node = functools.partial(agent_node, agent=rag_agent, name=\"RAGSearcher\")\n",
    "\n",
    "# search_agent = create_react_agent(llm, tools=[tavily_search_tool, web_scraper_tool, arxiv_search_tool])\n",
    "# search_node = functools.partial(agent_node, agent=search_agent, name=\"Searcher\")\n",
    "\n",
    "# code_agent = create_react_agent(llm, tools=[repl_tool])\n",
    "# code_node = functools.partial(agent_node, agent=search_agent, name=\"Coder\")\n",
    "\n",
    "# research_supervisor = create_team_supervisor(\n",
    "#     llm,\n",
    "#     \"\"\"\n",
    "#     Supervisor Instructions for Task Management\n",
    "\n",
    "#     You are assigned as the supervisor overseeing a conversation involving the following worker agents:\n",
    "    \n",
    "#     - **RAGSearcher**: This agent specializes in retrieving and analyzing relevant documents from the Qdrant Vector Store, handling specific document embedding searches. Use this agent to retrieve highly relevant sections of a document based on the user’s query, especially if the answer might lie within previously embedded resources.\n",
    "    \n",
    "#     - **Searcher**: This agent has access to web search and scraping tools, including **Tavily Search** for Google-based searches, **WebScraper** for targeted data extraction from specific web pages, and **ArxivSearch** for querying academic papers. Use the Searcher to gather new information from external sources or to clarify details that cannot be answered with existing documents.\n",
    "    \n",
    "#     - **Coder**: This agent has a code execution environment (**Python REPL tool**) and should be tasked with tasks that require data analysis, code-based computations, or visualizations. Utilize the Coder whenever a solution requires scripting, data processing, or creating custom visualizations.\n",
    "\n",
    "#     ### Guidelines for Task Management\n",
    "#     - **Efficiency**: Minimize search steps to save the user’s time by directing queries to the most relevant agent, avoiding redundant searches.\n",
    "#     - **Task Delegation**: Ensure tasks are appropriately delegated:\n",
    "#         - Use **RAGSearcher** when user queries involve pre-stored, embedded knowledge.\n",
    "#             ( - The following research paper texts are currently available in the vector database in embedded form: \"Attention is All You Need,\" \"MAGVIT: Masked Generative Video Transformer,\" and \"SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer.\")\n",
    "#         - Use **Searcher** when real-time data or unembedded references are required.\n",
    "#         - Use **Coder** for tasks that involve data processing or require executable code.\n",
    "#     - **Relevance and Specificity**: Tailor instructions for each agent based on the complexity and specificity of the task.\n",
    "#     - **Clear Communication**: Summarize findings and next steps for each agent’s outputs clearly before returning them to the user.\n",
    "\n",
    "#     ### Supervisor Guidelines\n",
    "#     - **Optimize Efficiency**: Select only the necessary tools to minimize the number of steps and save the user’s time.\n",
    "#     - **Focus on Relevance**: Prioritize agents based on the user’s specific needs to gather the most pertinent information.\n",
    "#     - **Conclude Tasks**: When all necessary information has been retrieved, conclude the workflow with **FINISH**.\n",
    "\n",
    "#     \"\"\",\n",
    "#     [\"RAGSearcher\", \"Searcher\", \"Coder\"],\n",
    "# )\n",
    "# research_team = StateGraph(ResearchTeamState)\n",
    "\n",
    "# # Add nodes for each agent in the research team\n",
    "# research_team.add_node('RAGSearcher', rag_node)\n",
    "# research_team.add_node('Searcher', search_node)\n",
    "# research_team.add_node('Coder', code_node)\n",
    "# research_team.add_node('ResearchSupervisor', research_supervisor)\n",
    "\n",
    "# # Define edges where each agent reports back to the supervisor\n",
    "# research_team.add_edge('RAGSearcher', 'ResearchSupervisor')\n",
    "# research_team.add_edge('Searcher', 'ResearchSupervisor')\n",
    "# research_team.add_edge('Coder', 'ResearchSupervisor')\n",
    "\n",
    "# # Add conditional edges based on the 'next' field in ResearchTeamState\n",
    "# research_team.add_conditional_edges(\n",
    "#     'ResearchSupervisor',\n",
    "#     lambda x: x['next'],\n",
    "#     {\"RAGSearcher\": \"RAGSearcher\", \"Searcher\": \"Searcher\", \"Coder\": \"Coder\", \"FINISH\": END}\n",
    "# )\n",
    "\n",
    "# # Define the starting point of the graph\n",
    "# research_team.add_edge(START, 'ResearchSupervisor')\n",
    "\n",
    "# # Compile the graph into a chain\n",
    "# research_team_compile = research_team.compile()\n",
    "# research_chain = enter_chain | research_team_compile \n",
    "\n",
    "# for s in research_chain.stream(\n",
    "#     \"what is the title of a.pdf?\", {\"recursion_limit\": 100}\n",
    "# ):\n",
    "#     if \"__end__\" not in s:\n",
    "#         print(s)\n",
    "#         print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## research team but wrong graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import (\n",
    "#     BaseMessage,\n",
    "#     HumanMessage,\n",
    "#     ToolMessage,\n",
    "#     AIMessage\n",
    "# )\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# def create_agent(llm, tools, system_message: str):\n",
    "#     \"\"\"Create an agent.\"\"\"\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"You are a helpful AI assistant.\"\n",
    "#                 \" Use the provided tools to progress towards answering the question.\"\n",
    "#                 \" Respond with 'FINISH' when done.\"\n",
    "#                 \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "#             ),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     prompt = prompt.partial(system_message=system_message)\n",
    "#     prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "#     return prompt | llm.bind_tools(tools)\n",
    "\n",
    "# import operator\n",
    "# from typing import Annotated, Sequence\n",
    "# from typing_extensions import TypedDict\n",
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# from typing import Literal\n",
    "\n",
    "\n",
    "# def route_tools(\n",
    "#     state: ResearchTeamState,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Use in the conditional_edge to route to the ToolNode if the last message\n",
    "#     has tool calls. Otherwise, route to the end.\n",
    "#     \"\"\"\n",
    "#     if isinstance(state, list):\n",
    "#         ai_message = state[-1]\n",
    "#     elif messages := state.get(\"messages\", []):\n",
    "#         ai_message = messages[-1]\n",
    "#     else:\n",
    "#         raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "#     if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "#         return \"tools\"\n",
    "#     return END\n",
    "\n",
    "\n",
    "\n",
    "# class ResearchTeamState(TypedDict):\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "#     team_members: List[str]\n",
    "#     next: str\n",
    "\n",
    "# rag_agent = create_agent(\n",
    "#     llm,\n",
    "#     [qdrant_retriever_tool],\n",
    "#     system_message=\"You should use qdrant_retriever_tool to retrieve accurate information from vector database.\",\n",
    "# )\n",
    "# rag_node = functools.partial(agent_node, agent=rag_agent, name=\"RagAgent\")\n",
    "\n",
    "# search_agent = create_agent(\n",
    "#     llm,\n",
    "#     [tavily_search_tool, arxiv_search_tool, web_scraper_tool],\n",
    "#     system_message=\"You should use relevant tool out of tavily_search_tool, arxiv_search_tool, web_scraper_tool to fetch information.\",\n",
    "# )\n",
    "# search_node = functools.partial(agent_node, agent=search_agent, name=\"SearchAgent\")\n",
    "\n",
    "# code_agent = create_agent(\n",
    "#     llm,\n",
    "#     [repl_tool],\n",
    "#     system_message=\"You should use this tool if you need to do the math or code anything, even to plot visualizations with matplotlib or seaborn.\",\n",
    "# )\n",
    "# code_node = functools.partial(agent_node, agent=code_agent, name=\"CodeAgent\")\n",
    "\n",
    "# research_supervisor = create_team_supervisor(\n",
    "#     llm,\n",
    "#     \"\"\"\n",
    "#     ### Supervisor Instructions for Task Management\n",
    "\n",
    "#     You are the supervisor responsible for coordinating the following specialized agents:\n",
    "\n",
    "#     - **RagAgent**: Utilizes the **Qdrant Retriever Tool** to fetch highly relevant information from a vector database. This agent is ideal for handling user queries that may be answered from pre-embedded content within the database. Deploy RagAgent when the user’s question is likely addressed by specific sections in stored documents.\n",
    "    \n",
    "#     - **SearchAgent**: Equipped with **Tavily Search**, **Arxiv Search**, and **WebScraper** tools, this agent performs external web and academic searches. Use SearchAgent to gather real-time information, explore academic references, or extract data from specific web pages as needed. This agent is suitable for expanding beyond embedded content or when additional clarification is needed.\n",
    "    \n",
    "#     - **CodeAgent**: Operates within a **Python REPL** environment, capable of handling code execution for data processing, mathematical calculations, or creating visualizations. Use CodeAgent whenever the task involves computational steps, data transformation, or graphical outputs.\n",
    "\n",
    "#     ### Task Delegation Guidelines\n",
    "#     - **Efficiency**: Prioritize relevant agents to streamline workflow and minimize unnecessary steps. \n",
    "#     - **Delegate by Task Type**:\n",
    "#         - Assign **RagAgent** for queries concerning content within the vector database.\n",
    "#         - Select **SearchAgent** for gathering new information or clarifying details that require web searches or external academic references.\n",
    "#         - Use **CodeAgent** when calculations, data analysis, or visualizations are necessary.\n",
    "#     - **Optimize for Relevance**: Tailor instructions based on the nature of the user’s query to ensure agents work with specificity and accuracy.\n",
    "\n",
    "#     ### Task Workflow and Communication\n",
    "#     - **Clear Summarization**: Summarize each agent's findings clearly and concisely, preparing the information to relay back to the user.\n",
    "#     - **End Task Flow**: Conclude the task sequence with **FINISH** once all required information has been obtained and is ready for the user.\n",
    "\n",
    "#     ### Current Embedded Documents\n",
    "#     - The following research papers are pre-embedded in the vector database for RagAgent to access:\n",
    "#         - \"Attention is All You Need\"\n",
    "#         - \"MAGVIT: Masked Generative Video Transformer\"\n",
    "#         - \"SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer\"\n",
    "\n",
    "#     \"\"\",\n",
    "#     [\"RagAgent\", \"SearchAgent\", \"CodeAgent\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# # Initialize StateGraph for the Research Team\n",
    "# research_team = StateGraph(ResearchTeamState)\n",
    "\n",
    "# # Add nodes for each agent and the supervisor\n",
    "# research_team.add_node(\"RagAgent\", rag_node)\n",
    "# research_team.add_node(\"SearchAgent\", search_node)\n",
    "# research_team.add_node(\"CodeAgent\", code_node)\n",
    "# research_team.add_node(\"ResearchSupervisor\", research_supervisor)\n",
    "\n",
    "# research_team.add_node(\"tools\", ToolNode(tools=[qdrant_retriever_tool, tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool]))\n",
    "\n",
    "# # Define edges where each agent reports back to the supervisor\n",
    "# research_team.add_edge(\"RagAgent\", \"ResearchSupervisor\")\n",
    "# research_team.add_edge(\"SearchAgent\", \"ResearchSupervisor\")\n",
    "# research_team.add_edge(\"CodeAgent\", \"ResearchSupervisor\")\n",
    "\n",
    "# # Add conditional edges based on the 'next' field in ResearchTeamState\n",
    "# research_team.add_conditional_edges(\n",
    "#     \"ResearchSupervisor\",\n",
    "#     lambda x: x[\"next\"],\n",
    "#     {\"RagAgent\": \"RagAgent\", \"SearchAgent\": \"SearchAgent\", \"CodeAgent\": \"CodeAgent\", \"FINISH\": END}\n",
    "# )\n",
    "\n",
    "# research_team.add_conditional_edges(\n",
    "#     \"RagAgent\",\n",
    "#     route_tools,\n",
    "#     {\"tools\": \"tools\", END: END}\n",
    "# )\n",
    "\n",
    "# research_team.add_conditional_edges(\n",
    "#     \"SearchAgent\",\n",
    "#     route_tools,\n",
    "#     {\"tools\": \"tools\", END: END}\n",
    "# )\n",
    "\n",
    "# research_team.add_conditional_edges(\n",
    "#     \"CodeAgent\",\n",
    "#     route_tools,\n",
    "#     {\"tools\": \"tools\", END: END}\n",
    "# )\n",
    "\n",
    "# research_team.add_edge(\"tools\", \"RagAgent\")\n",
    "# research_team.add_edge(\"tools\", \"SearchAgent\")\n",
    "# research_team.add_edge(\"tools\", \"CodeAgent\")\n",
    "\n",
    "# # Define the starting point of the graph\n",
    "# research_team.add_edge(START, \"ResearchSupervisor\")\n",
    "\n",
    "# # Compile the graph into a chain\n",
    "# research_team_compile = research_team.compile()\n",
    "# research_chain = enter_chain | research_team_compile\n",
    "\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(research_team_compile.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "\n",
    "# for s in research_team_compile.stream(\n",
    "#     {'messages': [HumanMessage(\n",
    "#                  content=\"what is attention mechanism\"\n",
    "#             )]}\n",
    "# ):\n",
    "#     if \"__end__\" not in s:\n",
    "#         print(s)\n",
    "#         print(\"---\")\n",
    "\n",
    "# for tool_call in a.tool_calls:\n",
    "#     selected_tool = {\"retrieve_research_paper_texts\": qdrant_retriever}[tool_call[\"name\"].lower()]\n",
    "#     tool_msg = selected_tool.invoke(tool_call)\n",
    "#     print(tool_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## research team but with rewoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "For the following task, create a series of plans that can solve the problem step by step. Each plan should break down the task into smaller, simpler sub-questions that efficiently utilize the available tools. Stick to the context of the original question to ensure clarity and coherence in the steps. Each plan should use the minimum number of steps necessary to provide a sufficient answer to the user.\n",
    "\n",
    "For each plan, specify the external tool to retrieve the necessary evidence and store this evidence in a variable #E that can be used by later tools (e.g., Plan, #E1, Plan, #E2, Plan, ...).\n",
    "\n",
    "Tools available:\n",
    "qdrant_retriever_tool[input]: Retrieves documents or relevant research papers from an embedded vector database, useful for finding answers based on prior knowledge or previously indexed content (use when the task relates to research papers).\n",
    "repl_tool[input]: A Python REPL (Read-Eval-Print Loop) for executing code, data analysis, or visualizations.\n",
    "arxiv_search_tool[input]: Searches for academic papers and research on Arxiv. Use it to gather specific research papers or academic studies.\n",
    "tavily_search_tool[input]: Performs Google searches, useful for retrieving quick information from external websites and sources (use for general web-based queries).\n",
    "\n",
    "Routing Guidance:\n",
    "For questions related to research papers or prior knowledge, use the qdrant_retriever_tool to retrieve evidence from the vectorstore.\n",
    "For other questions (especially those requiring external web searches or specific data from web pages), use tavily_search_tool, arxiv_search_tool, or web_scraper_tool as appropriate.\n",
    "\n",
    "Each plan should be followed by only one #E.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Task: Retrieve the latest news on the Australian Open.\n",
    "   Plan: Search for the latest updates on the 2024 Australian Open via Tavily Search. #E1 = tavily_search_tool[2024 Australian Open news]\n",
    "   Plan: Retrieve additional information from relevant research papers if available in the vector store using RAG search. #E2 = qdrant_retriever_tool[Australian Open 2024]\n",
    "   Plan: Use the gathered information to generate a summary of the latest updates. #E3 = LLM[Summarize #E1, #E2]\n",
    "\n",
    "Task: Find the latest advancements in Generative AI models.\n",
    "   Plan: Search for recent papers on Arxiv related to Generative AI advancements. #E1 = arxiv_search_tool[Generative AI advancements]\n",
    "   Plan: Retrieve relevant research papers related to Generative AI from the vector database using RAG search. #E2 = qdrant_retriever_tool[Generative AI]\n",
    "   Plan: Use the retrieved papers to summarize key insights and developments. #E3 = LLM[Summarize #E1, #E2]\n",
    "\n",
    "Begin! Provide only necessary output and nothing else. Create a plan such that each sub-question is answered.\n",
    "Please create the minimum number of plans necessary to answer the task efficiently. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}\n",
    "\"\"\"\n",
    "task = \"what is attention mecahnism, and also explain multi collinearity, explain hallucaination to a child in context of deep learning\"\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pprint import pprint\n",
    "\n",
    "class ReWOO(TypedDict):\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str\n",
    "    \n",
    "result = llm.invoke(prompt.format(task=task))   \n",
    "pprint(result.content)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Regex to match expressions of the form E#... = ...[...]\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", prompt)])\n",
    "planner = prompt_template | trimmer | llm\n",
    "\n",
    "\n",
    "def get_plan(state: ReWOO):\n",
    "    task = state[\"task\"]\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    # Find all matches in the sample text\n",
    "    matches = re.findall(regex_pattern, result.content)\n",
    "    return {\"steps\": matches, \"plan_string\": result.content}\n",
    "\n",
    "def _get_current_task(state: ReWOO):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "def tool_execution(state: ReWOO):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    _, step_name, tool, tool_input = state[\"steps\"][_step - 1]\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    \n",
    "    # Replace placeholders in tool_input with results from previous steps\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "    \n",
    "    # Execute the correct tool based on the provided tool name\n",
    "    if tool == \"tavily_search_tool\":\n",
    "        result = tavily_search_tool.invoke(tool_input)\n",
    "    elif tool == \"arxiv_search_tool\":\n",
    "        result = arxiv_search_tool.invoke(tool_input)\n",
    "    elif tool == \"repl_tool\":\n",
    "        result = repl_tool.invoke(tool_input)\n",
    "    elif tool == \"qdrant_retriever_tool\":\n",
    "        result = qdrant_retriever_tool.invoke(tool_input)\n",
    "    elif tool == \"LLM\":\n",
    "        result = llm.invoke(tool_input)  # assuming 'llm' is your pretrained LLM object\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tool: {tool}\")\n",
    "    \n",
    "    # Store the result in the state\n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}\n",
    "\n",
    "solve_prompt = \"\"\"Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. After going through all answers provide short answers to each question, in w to 3 lines.\n",
    "\n",
    "Task: {task}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ReWOO):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool, tool_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool}[{tool_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content}\n",
    "\n",
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        # We have executed all tasks\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        # We are still executing tasks, loop back to the \"tool\" node\n",
    "        return \"tool\"\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Define your plan and solve functions (not shown in your code, assuming they're defined elsewhere)\n",
    "graph = StateGraph(ReWOO)\n",
    "graph.add_node(\"plan\", get_plan)\n",
    "graph.add_node(\"tool\", tool_execution)\n",
    "graph.add_node(\"solve\", solve)\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# for s in app.stream({\"task\": task}):\n",
    "#     print(s)\n",
    "#     print(\"---\")\n",
    "\n",
    "# # Print out the final result\n",
    "# pprint(s[\"solve\"][\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic react"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import (\n",
    "#     BaseMessage,\n",
    "#     HumanMessage,\n",
    "#     ToolMessage,\n",
    "# )\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "# def create_agent(llm, tools, system_message: str):\n",
    "#     \"\"\"Create an agent.\"\"\"\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "#                 \" Use the provided tools to progress towards answering the question.\"\n",
    "#                 \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "#                 \" will help where you left off. Execute what you can to make progress.\"\n",
    "#                 \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "#                 \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "#                 \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "#             ),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     prompt = prompt.partial(system_message=system_message)\n",
    "#     prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "#     return prompt | llm.bind_tools(tools)\n",
    "\n",
    "# import operator\n",
    "# from typing import Annotated, Sequence\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# # This defines the object that is passed between each node\n",
    "# # in the graph. We will create different nodes for each agent and tool\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "#     sender: str\n",
    "\n",
    "# import functools\n",
    "\n",
    "# from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# # Helper function to create a node for a given agent\n",
    "# def agent_node(state, agent, name):\n",
    "#     result = agent.invoke(state)\n",
    "#     # We convert the agent output into a format that is suitable to append to the global state\n",
    "#     if isinstance(result, ToolMessage):\n",
    "#         pass\n",
    "#     else:\n",
    "#         result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "#     return {\n",
    "#         \"messages\": [result],\n",
    "#         # Since we have a strict workflow, we can\n",
    "#         # track the sender so we know who to pass to next.\n",
    "#         \"sender\": name,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Research agent and node\n",
    "# research_agent = create_agent(\n",
    "#     llm,\n",
    "#     [tavily_search_tool, web_scraper_tool, arxiv_search_tool],\n",
    "#     system_message=\"You should provide accurate data for the chart_generator to use.\",\n",
    "# )\n",
    "# research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# # Qdrant retriever agent and node\n",
    "# qdrant_retriever_agent = create_agent(\n",
    "#     llm,\n",
    "#     [qdrant_retriever_tool],\n",
    "#     system_message=\"Retrieve and analyze the most relevant information.\",\n",
    "# )\n",
    "# qdrant_retriever_node = functools.partial(agent_node, agent=qdrant_retriever_agent, name=\"qdrant_retriever\")\n",
    "\n",
    "# # Chart generator agent and node\n",
    "# chart_agent = create_agent(\n",
    "#     llm,\n",
    "#     [repl_tool],\n",
    "#     system_message=\"Any charts you display will be visible by the user.\",\n",
    "# )\n",
    "# chart_node = functools.partial(agent_node, agent=chart_agent, name=\"chart_generator\")\n",
    "\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# # Single ToolNode with all tools combined\n",
    "# tool_node = ToolNode([\n",
    "#     tavily_search_tool, \n",
    "#     web_scraper_tool, \n",
    "#     arxiv_search_tool, \n",
    "#     qdrant_retriever_tool, \n",
    "#     repl_tool\n",
    "# ])\n",
    "\n",
    "# # Either agent can decide to end\n",
    "# from typing import Literal\n",
    "\n",
    "\n",
    "# def router(state):\n",
    "#     # This is the router\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "#     if last_message.tool_calls:\n",
    "#         # The previous agent is invoking a tool\n",
    "#         return \"call_tool\"\n",
    "#     if \"FINAL ANSWER\" in last_message.content:\n",
    "#         # Any agent decided the work is done\n",
    "#         return END\n",
    "#     return \"continue\"\n",
    "\n",
    "# workflow = StateGraph(AgentState)\n",
    "\n",
    "# # Add nodes for each agent\n",
    "# workflow.add_node(\"Researcher\", research_node)  # Research agent with Tavily, Web Scraper, Arxiv tools\n",
    "# workflow.add_node(\"chart_generator\", chart_node)  # Chart generator agent with REPL tool\n",
    "# workflow.add_node(\"qdrant_retriever\", qdrant_retriever_node)  # Qdrant retriever agent\n",
    "\n",
    "# # Add a single tool node combining all tools\n",
    "# workflow.add_node(\"all_tools\", tool_node)\n",
    "\n",
    "# # Define the conditional edges for routing between nodes\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"Researcher\",\n",
    "#     router,\n",
    "#     {\"continue\": \"chart_generator\", \"call_tool\": \"all_tools\", END: END},\n",
    "# )\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"chart_generator\",\n",
    "#     router,\n",
    "#     {\"continue\": \"Researcher\", \"call_tool\": \"all_tools\", END: END},\n",
    "# )\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"qdrant_retriever\",\n",
    "#     router,\n",
    "#     {\"continue\": \"Researcher\", \"call_tool\": \"all_tools\", END: END},\n",
    "# )\n",
    "\n",
    "# # Routing back to the invoking agent\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"all_tools\",\n",
    "#     # Route back to the original agent that invoked the tool\n",
    "#     lambda x: x[\"sender\"],\n",
    "#     {\n",
    "#         \"Researcher\": \"Researcher\",\n",
    "#         \"chart_generator\": \"chart_generator\",\n",
    "#         \"qdrant_retriever\": \"qdrant_retriever\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Define the starting point of the workflow\n",
    "# workflow.add_edge(START, \"Researcher\")\n",
    "\n",
    "# # Compile the workflow graph\n",
    "# graph = workflow.compile()\n",
    "\n",
    "# # from IPython.display import Image, display\n",
    "\n",
    "# # try:\n",
    "# #     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# # except Exception:\n",
    "# #     # This requires some extra dependencies and is optional\n",
    "# #     pass\n",
    "\n",
    "\n",
    "# events = graph.stream(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(\n",
    "#                 content='what is attention, how can i explain it to a 60 year old, also tell exlain me the abstratc of a.pdf'\n",
    "#             )\n",
    "#         ],\n",
    "#     },\n",
    "#     # Maximum number of steps to take in the graph\n",
    "#     {\"recursion_limit\": 150},\n",
    "# )\n",
    "# for s in events:\n",
    "#     print(s)\n",
    "#     print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## research team with wrong reWoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # Regex to match expressions of the form E#... = ...[...]\n",
    "# regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "# prompt_template = ChatPromptTemplate.from_messages([(\"user\", prompt)])\n",
    "# planner = prompt_template | llm\n",
    "\n",
    "\n",
    "# def get_plan(state: ReWOO):\n",
    "#     task = state[\"task\"]\n",
    "#     result = planner.invoke({\"task\": task})\n",
    "#     # Find all matches in the sample text\n",
    "#     matches = re.findall(regex_pattern, result.content)\n",
    "#     return {\"steps\": matches, \"plan_string\": result.content}\n",
    "\n",
    "# def _get_current_task(state: ReWOO):\n",
    "#     if \"results\" not in state or state[\"results\"] is None:\n",
    "#         return 1\n",
    "#     if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "#         return None\n",
    "#     else:\n",
    "#         return len(state[\"results\"]) + 1\n",
    "\n",
    "# def agent_execution(state: ReWOO):\n",
    "#     \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    \n",
    "#     # Get the current task step\n",
    "#     _step = _get_current_task(state)\n",
    "#     _, step_name, agent, agent_input = state[\"steps\"][_step - 1]\n",
    "\n",
    "#     # Prepare the results dictionary\n",
    "#     _results = state.get(\"results\", {})\n",
    "    \n",
    "#     # Substitute placeholders in agent_input with values from _results\n",
    "#     for k, v in _results.items():\n",
    "#         agent_input = agent_input.replace(k, v)\n",
    "\n",
    "#     # Execute the correct agent based on the specified tool\n",
    "#     if agent == \"RAGSearcher\":\n",
    "#         result = rag_agent({'messages': ('user', agent_input)})  # Use rag_agent for RAGSearcher tasks\n",
    "#     elif agent == \"Searcher\":\n",
    "#         result = search_agent({'messages': ('user', agent_input)})  # Use search_agent for Searcher tasks\n",
    "#     elif agent == \"Coder\":\n",
    "#         result = code_agent({'messages': ('user', agent_input)})  # Use code_agent for Coder tasks\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown agent type: {agent}\")\n",
    "\n",
    "#     # Store the result for this step\n",
    "#     _results[step_name] = str(result)\n",
    "\n",
    "#     # Return the updated results dictionary\n",
    "#     return {\"results\": _results}\n",
    "\n",
    "\n",
    "# def solve(state: ReWOO):\n",
    "#     plan = \"\"\n",
    "    \n",
    "#     for _plan, step_name, agent, agent_input in state[\"steps\"]:\n",
    "#         # Prepare results dictionary (if any)\n",
    "#         _results = state.get(\"results\", {})\n",
    "\n",
    "#         # Replace placeholders in the agent_input and step_name with values from results\n",
    "#         for k, v in _results.items():\n",
    "#             agent_input = agent_input.replace(k, v)\n",
    "#             step_name = step_name.replace(k, v)\n",
    "\n",
    "#         # Based on the agent, call the corresponding agent node and prepare the plan\n",
    "#         if agent == \"RAGSearcher\":\n",
    "#             plan += f\"Plan: {_plan}\\n{step_name} = {rag_node}[{'messages': agent_input}]\\n\"\n",
    "#         elif agent == \"Searcher\":\n",
    "#             plan += f\"Plan: {_plan}\\n{step_name} = {search_node}[{'messages': agent_input}\\n\"\n",
    "#         elif agent == \"Coder\":\n",
    "#             plan += f\"Plan: {_plan}\\n{step_name} = {code_node}[{'messages': agent_input}]\\n\"\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown agent type: {agent}\")\n",
    "\n",
    "#     # Prepare the prompt with the constructed plan\n",
    "#     prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    \n",
    "#     # Invoke the LLM with the formatted prompt\n",
    "#     result = model.invoke(prompt)\n",
    "\n",
    "#     # Return the result from the LLM\n",
    "#     return {\"result\": result.content}\n",
    "\n",
    "# def _route(state):\n",
    "#     _step = _get_current_task(state)\n",
    "#     if _step is None:\n",
    "#         # We have executed all tasks\n",
    "#         return \"solve\"\n",
    "#     else:\n",
    "#         # We are still executing tasks, loop back to the \"tool\" node\n",
    "#         return \"tool\"\n",
    "\n",
    "\n",
    "# from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# # Define the StateGraph with ReWOO as the context or state manager\n",
    "# graph = StateGraph(ReWOO)\n",
    "\n",
    "# # Add the nodes (steps in the process)\n",
    "# graph.add_node(\"plan\", get_plan)  # This node generates the plan for the workflow\n",
    "# graph.add_node(\"tool\", agent_execution)  # This node executes the tools defined in the plan\n",
    "# graph.add_node(\"solve\", solve)  # This node solves the problem using the generated plan\n",
    "\n",
    "# # Add edges between the nodes\n",
    "# graph.add_edge(\"plan\", \"tool\")  # After the plan is generated, execute the tools\n",
    "# graph.add_edge(\"tool\", \"solve\")  # After tool execution, the result is passed to solve\n",
    "# graph.add_edge(\"solve\", END)  # Once solved, we reach the end of the workflow\n",
    "\n",
    "# # Add conditional edges based on specific conditions or routing logic\n",
    "# graph.add_conditional_edges(\"tool\", _route)  # The routing logic defines what happens after the tool execution\n",
    "\n",
    "# # Define the starting point of the workflow\n",
    "# graph.add_edge(START, \"plan\")\n",
    "\n",
    "# # Compile the graph to create the workflow application\n",
    "# app = graph.compile()\n",
    "\n",
    "# for s in app.stream({\"task\": task}):\n",
    "#     print(s)\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modification  of next  plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_plan(m):\n",
    "#     task = m\n",
    "#     result = planner.invoke(task)\n",
    "    \n",
    "#     matches = re.findall(regex_pattern, result.content)\n",
    "\n",
    "#     # Convert the tuple output to a dictionary list\n",
    "#     steps = [\n",
    "#         {\n",
    "#             \"description\": match[0],  # Plan description\n",
    "#             \"step_id\": match[1],      # Step identifier\n",
    "#             \"agent\": match[2],        # Agent used\n",
    "#             \"input_data\": match[3]    # Input data or reference\n",
    "#         }\n",
    "#         for match in matches\n",
    "#     ]\n",
    "    \n",
    "#     return {\"steps\": steps, \"plan_string\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rewoo + hierarchy failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # utils\n",
    "# ## router\n",
    "# def create_team_supervisor(llm, system_prompt, members) -> str:\n",
    "#     \"\"\"An LLM-based supervisor router for ReWOO-style reasoning.\"\"\"\n",
    "#     options = [\"FINISH\"] + members\n",
    "    \n",
    "#     # Define a function schema for routing steps with detailed 'step' information\n",
    "#     function_def = {\n",
    "#         \"name\": \"route\",\n",
    "#         \"description\": \"Execute the steps of a given plan sequentially by invoking the specified agent.\",\n",
    "#         \"parameters\": {\n",
    "#             \"title\": \"routeSchema\",\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"next\": {\n",
    "#                     \"title\": \"Next Agent\",\n",
    "#                     \"anyOf\": [\n",
    "#                         {\"enum\": options},\n",
    "#                     ],\n",
    "#                 },\n",
    "#                 \"step\": {\n",
    "#                     \"title\": \"Step Description\",\n",
    "#                     \"type\": \"object\",  # Change from string to object to include detailed fields\n",
    "#                     \"properties\": {\n",
    "#                         \"description\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                             \"description\": \"The description of the step to execute.\"\n",
    "#                         },\n",
    "#                         \"step_id\": {\n",
    "#                             \"type\" : \"string\",\n",
    "#                             \"description\": \"Step identifier. Eg. #E[number]\"\n",
    "#                         },\n",
    "#                         \"agent\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                             \"description\": \"The agent to use for this step.\"\n",
    "#                         },\n",
    "#                         \"input\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                             \"description\": \"The input required for the agent in this step.\"\n",
    "#                         }\n",
    "#                     },\n",
    "#                     \"required\": [\"description\", \"step_id\", \"agent\", \"input\"]\n",
    "#                 },\n",
    "#                 # \"input\": {\n",
    "#                 #     \"title\": \"Agent Input\",\n",
    "#                 #     \"type\": \"string\",\n",
    "#                 #     \"description\": \"The exact input required for the agent in this step.\",\n",
    "#                 # },\n",
    "#             },\n",
    "#             \"required\": [\"next\", \"step\"],\n",
    "#         },\n",
    "#     }\n",
    "    \n",
    "#     # Define a system prompt to guide the supervisor's behavior\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system_prompt),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"Based on the conversation and plan, determine who should act next. \"\n",
    "#                 \"Provide the step to execute, the agent to use, and the input for the agent. \"\n",
    "#                 \"Select one of: {options}. Use FINISH when all steps are complete.\"\n",
    "#             ),\n",
    "#         ]\n",
    "#     ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    \n",
    "#     # Combine with trimmer, bind functions, and output parser\n",
    "#     return (\n",
    "#         prompt\n",
    "#         | trimmer\n",
    "#         | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "#         | JsonOutputFunctionsParser()\n",
    "#     )\n",
    "\n",
    "# ## basic utils\n",
    "# def agent_node(state, agent, name):\n",
    "#     result = agent.invoke(state)\n",
    "#     return {\n",
    "#         \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\n",
    "#     }\n",
    "\n",
    "# def enter_chain(message: str):\n",
    "#     decomposed_query = dec.invoke(message)\n",
    "\n",
    "#     results = {\n",
    "#         \"messages\": [HumanMessage(content=message)],\n",
    "#         \"task\": decomposed_query\n",
    "#     }\n",
    "#     return results\n",
    "# # state\n",
    "# from typing import List, Dict\n",
    "# from typing_extensions import TypedDict\n",
    "# from pprint import pprint\n",
    "\n",
    "# class ResearchTeamState(TypedDict):\n",
    "#     messages: Annotated[List[BaseMessage], operator.add]\n",
    "#     team_members: List[str]\n",
    "\n",
    "#     task: str\n",
    "#     plan_string: str\n",
    "#     steps: List[Dict[str, str, str, str]]\n",
    "#     results: dict\n",
    "#     result: str\n",
    "\n",
    "#     next: str\n",
    "#     step: str\n",
    "# # orch\n",
    "# import re\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "# prompt_template = ChatPromptTemplate.from_messages([(\"user\", rewoo_prompt)])\n",
    "# planner = prompt_template | llm\n",
    "\n",
    "\n",
    "# def get_plan(state: ResearchTeamState):\n",
    "#     task = state[\"task\"]\n",
    "#     result = planner.invoke({\"task\": task})\n",
    "#     # Find all matches in the sample text\n",
    "#     matches = re.findall(regex_pattern, result.content)\n",
    "\n",
    "#     # Convert the tuple output to a dictionary list\n",
    "#     steps = [\n",
    "#         {\n",
    "#             \"description\": match[0],  # Plan description\n",
    "#             \"step_id\": match[1],      # Step identifier\n",
    "#             \"agent\": match[2],        # Agent used\n",
    "#             \"input_data\": match[3]    # Input data or reference\n",
    "#         }\n",
    "#         for match in matches\n",
    "#     ]\n",
    "    \n",
    "#     return {\"steps\": steps, \"plan_string\": result.content}\n",
    "\n",
    "# def _get_current_task(state: ResearchTeamState):\n",
    "#     if \"results\" not in state or state[\"results\"] is None:\n",
    "#         return 1\n",
    "#     if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "#         return None\n",
    "#     else:\n",
    "#         return len(state[\"results\"]) + 1\n",
    "# rag_agent = create_react_agent(llm, tools=[qdrant_retriever_tool])\n",
    "# rag_node = functools.partial(agent_node, agent=rag_agent, name=\"RAGSearcher\")\n",
    "\n",
    "# search_agent = create_react_agent(llm, tools=[tavily_search_tool, web_scraper_tool, arxiv_search_tool])\n",
    "# search_node = functools.partial(agent_node, agent=search_agent, name=\"Searcher\")\n",
    "\n",
    "# code_agent = create_react_agent(llm, tools=[repl_tool])\n",
    "# code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "# # Define the chatbot function\n",
    "# def chatbot(state: ResearchTeamState):\n",
    "#     return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# # Define the supervisor agent\n",
    "# supervisor_agent = create_team_supervisor(\n",
    "#     llm,\n",
    "#     system_prompt=(\n",
    "#         \"You are a supervisor tasked with managing and orchestrating a team of workers: \"\n",
    "#         \"RAGSearcher, Searcher, Coder, and ChatBot. Your goal is to execute the provided plan sequentially. \"\n",
    "#         \"Each worker will perform a task as per the step description and respond with their results and status. \"\n",
    "#         \"When all steps are complete, respond with FINISH.\\n\\n\"\n",
    "#         \"At each step:\\n\"\n",
    "#         \"- Identify the next agent to act based on the plan.\\n\"\n",
    "#         \"- Specify the step to execute and the required input.\\n\"\n",
    "#         \"- Continue until all steps are completed.\\n\"\n",
    "#         \"Ensure the responses are concise, relevant, and aligned with the task.\"\n",
    "#     ),\n",
    "#     members=[\"RAGSearcher\", \"Searcher\", \"Coder\", \"ChatBot\"]\n",
    "# )\n",
    "\n",
    "# # Initialize the state graph for the research process\n",
    "# research_graph = StateGraph(ResearchTeamState)\n",
    "\n",
    "# # Add nodes for each agent\n",
    "# research_graph.add_node(\"Search\", search_node)\n",
    "# research_graph.add_node(\"WebScraper\", research_node)  # Assuming this is an existing function for the web scraper agent\n",
    "# research_graph.add_node(\"supervisor\", supervisor_agent)\n",
    "# research_graph.add_node(\"ChatBot\", chatbot)  # Adding the ChatBot node\n",
    "\n",
    "# # Define the control flow (edges)\n",
    "# research_graph.add_edge(\"Search\", \"supervisor\")\n",
    "# research_graph.add_edge(\"WebScraper\", \"supervisor\")\n",
    "# research_graph.add_edge(\"RAGSearcher\", \"supervisor\")\n",
    "# research_graph.add_edge(\"Coder\", \"supervisor\")\n",
    "# research_graph.add_edge(\"ChatBot\", \"supervisor\")  # Ensure ChatBot is also routed through the supervisor\n",
    "\n",
    "# # Add conditional edges based on supervisor's decisions\n",
    "# research_graph.add_conditional_edges(\n",
    "#     \"supervisor\",\n",
    "#     lambda x: x[\"next\"],  # Determine the next step based on supervisor's output\n",
    "#     {\n",
    "#         \"Search\": \"Search\",\n",
    "#         \"WebScraper\": \"WebScraper\",\n",
    "#         \"RAGSearcher\": \"RAGSearcher\",\n",
    "#         \"ChatBot\": \"ChatBot\",\n",
    "#         \"Coder\": \"Coder\",\n",
    "#         \"FINISH\": END  # The end state when all steps are finished\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Add the initial edge to start the process\n",
    "# research_graph.add_edge(START, \"supervisor\")\n",
    "\n",
    "# # Compile the entire research chain\n",
    "# chain = research_graph.compile()\n",
    "\n",
    "# # Function to enter the chain state and pass initial input\n",
    "# def enter_chain(message: str):\n",
    "#     results = {\n",
    "#         \"messages\": [HumanMessage(content=message)],\n",
    "#     }\n",
    "#     return results\n",
    "\n",
    "# # Now you can invoke the research chain with the entered message\n",
    "# research_chain = enter_chain | chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewoo team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26540/1608887471.py:39: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  qdrant_retriever_tool = qdrant_retriever.as_tool(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition of fully homomorphic encryption\n",
      "what is fully homomorphic encryption?, where is fully homomorphic encryption typically applied or used?, what are the current advancements or recent research developments in fully homomorphic encryption?\n",
      "Plan: Search for information on fully homomorphic encryption using the Searcher agent to gather general knowledge from online sources. #E1 = Searcher[fully homomorphic encryption definition]\n",
      "Plan: Retrieve research papers and academic articles related to fully homomorphic encryption from Arxiv and the vector database using the Searcher and RagSearcher agents. #E2 = Searcher[fully homomorphic encryption research papers], #E3 = RagSearcher[fully homomorphic encryption]\n",
      "Plan: Summarize the definition and key concepts of fully homomorphic encryption based on the retrieved information. #E4 = ChatBot[Summarize #E1, #E2, #E3]\n",
      "{'decompose_or_rephrase': {'task': 'define fully homomorphic encryption.'}}\n",
      "---\n",
      "{'get_plan': {'steps': [('Search for academic papers and web information on fully homomorphic encryption to gather definitions and explanations using the Searcher agent. ', '#E1', 'Searcher', 'fully homomorphic encryption definition'), ('Retrieve related research papers on fully homomorphic encryption stored in the vector database using the RagSearcher agent. ', '#E2', 'RagSearcher', 'fully homomorphic encryption definition'), ('Use the retrieved documents to generate a summary and definition of fully homomorphic encryption. ', '#E3', 'ChatBot', 'Summarize #E1, #E2')]}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.'}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The definition of fully homomorphic encryption (FHE) is not explicitly mentioned in the provided text. However, I can provide a general definition of FHE:\\n\\nFully homomorphic encryption is a type of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. In other words, FHE enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the plaintext (unencrypted data). This allows data to be processed and analyzed while maintaining its confidentiality.'}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The definition of fully homomorphic encryption (FHE) is not explicitly mentioned in the provided text. However, I can provide a general definition of FHE:\\n\\nFully homomorphic encryption is a type of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. In other words, FHE enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the plaintext (unencrypted data). This allows data to be processed and analyzed while maintaining its confidentiality.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain a definition of fully homomorphic encryption.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'Based on the search results, I was unable to find the definition for \"fully homomorphic encryption\".', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'Based on the search results, there is no definition for \"fully homomorphic encryption\" in the provided text. However, I can provide a general definition for fully homomorphic encryption:\\n\\nFully homomorphic encryption (FHE) is a type of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. This means that data can be processed and analyzed in its encrypted form, without revealing the underlying plaintext (unencrypted data). FHE is considered a powerful tool for secure computing, as it enables secure outsourcing of computations to untrusted parties, such as cloud providers, while maintaining the confidentiality of the data.\\n\\nIf you would like to know more about encryption, I can try to provide you with more information or search for a different topic.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain information about fully homomorphic encryption. The search results appear to be related to a research paper about a method called EchoMimicV2, which is used for half-body human animation.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The query \"fully homomorphic encryption definition\" was not answered by the search results.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain a definition of fully homomorphic encryption. The text appears to be a research paper on the topic of audio-driven human animation, specifically discussing the EchoMimicV2 method. It does not mention fully homomorphic encryption.\\n\\nIf you would like to search for the definition of fully homomorphic encryption, I can try to assist you with that.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'Fully homomorphic encryption is a cryptographic technique that allows computations to be performed on encrypted data without decrypting it first.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'It seems like the function is not working as expected. I will change the approach and provide a general definition of homomorphic encryption.\\n\\nHomomorphic encryption is a form of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. This means that encrypted data can be processed and analyzed without compromising the security of the data, as the computations are performed directly on the encrypted data.\\n\\nHowever, if you are looking for a more specific definition or information related to fully homomorphic encryption, I can try to provide more general information.\\n\\nFully homomorphic encryption (FHE) is a type of homomorphic encryption that allows for both addition and multiplication operations to be performed on encrypted data. This means that any computation can be performed on the encrypted data without decrypting it first, making it a powerful tool for secure data processing and analysis.\\n\\nIf you would like to know more about fully homomorphic encryption or any other related topic, please let me know.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'I apologize for the inconvenience. It seems like the function is not working correctly. I will try to provide a general definition of fully homomorphic encryption instead.\\n\\nFully homomorphic encryption (FHE) is a type of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. This means that any computation can be performed on the encrypted data, and the result will be the same as if the computation had been performed on the plaintext (unencrypted data). This allows for secure outsourcing of computations to untrusted parties, such as cloud computing services, without compromising the security of the data.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'I apologize, but I cannot provide a full definition as the function calls to provide the definition directly were unsuccessful.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The definition of fully homomorphic encryption is not provided in the given text. However, the text appears to be related to a research paper on a novel end-to-end method for portrait animation, called EchoMimicV2, which uses Audio-Pose Dynamic Harmonization and Phase-specific Loss to enhance motion, details, and low-level quality.\\n\\nTo get a definition for fully homomorphic encryption, another search would be needed.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided texts do not contain information about fully homomorphic encryption (FHE). The texts appear to be related to computer vision, machine learning, and animation, but do not mention FHE. If you would like to know more about FHE, I can provide general information or try to retrieve more relevant texts.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The search results did not contain the requested information on fully homomorphic encryption definition.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain the requested definition of fully homomorphic encryption.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain a definition of fully homomorphic encryption. The text appears to be related to a research paper on the topic of human animation and audio-driven animation, and does not mention fully homomorphic encryption.\\n\\nIf you would like to search for the definition of fully homomorphic encryption, I can try searching again.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The search results did not contain the requested information on fully homomorphic encryption definition.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The concept of fully homomorphic encryption (FHE) is not explicitly mentioned in the provided text snippets. However, I can provide a general definition of FHE:\\n\\nFully homomorphic encryption is a form of encryption that allows computations to be performed on ciphertext (encrypted data) without decrypting it first. This means that any computation that can be performed on plaintext (unencrypted data) can also be performed on ciphertext, without revealing the underlying data. FHE has the potential to enable secure outsourcing of computations to untrusted parties, such as cloud computing services, while maintaining the confidentiality of the data being processed.\\n\\nIf you would like to search for information on fully homomorphic encryption using a different search query, I can try to assist you further.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The definition of fully homomorphic encryption could not be found in the search results.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain information about fully homomorphic encryption. The search results are related to papers about human animation and video generation using audio and pose conditions.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n",
      "{'agent_exec': {'results': {'#E1': 'Fully homomorphic encryption is an encryption method that allows computations to be performed on encrypted data without decrypting it first. It is a cryptographic technique that enables computations on encrypted data while maintaining the confidentiality of the data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) is a technique that allows computations on encrypted quantum data.', '#E2': 'The provided text does not contain information about fully homomorphic encryption. The text appears to be related to computer science and machine learning, specifically discussing methods for generating videos of humans based on audio inputs.', '#E3': \"content='Fully homomorphic encryption (FHE) allows computations to be performed on encrypted data without decrypting it first. This technique enables computations on encrypted data, generating an encrypted result that, when decrypted, matches the result of the same computation on the unencrypted data. In the context of quantum information processing, quantum fully homomorphic encryption (QFHE) extends this concept to computations on encrypted quantum data, maintaining the confidentiality of the data throughout the process.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 217, 'total_tokens': 308, 'completion_time': 0.449538005, 'prompt_time': 0.035612164, 'queue_time': 0.011916466, 'total_time': 0.485150169}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None} id='run-56bf0905-a54d-464d-a265-97b57b8a32f5-0' usage_metadata={'input_tokens': 217, 'output_tokens': 91, 'total_tokens': 308}\"}}}\n",
      "---\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 225\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# from IPython.display import display, Image\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# display(Image(research_graph.get_graph().draw_mermaid_png()))\u001b[39;00m\n\u001b[1;32m    224\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresearch_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is fully homomorphic encryption.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m---\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1348\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1340\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   1341\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1342\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[0;32m-> 1348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, List, Optional, Union, Dict, Annotated, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, trim_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from team_tools import tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "from llm_chains import decomposition_chain, requires_decomposition, rephrase_chain, get_plan_chain\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from token_counter import tiktoken_counter\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview')\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=5984,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles\",\n",
    ")\n",
    "# state\n",
    "decomposer_chain = decomposition_chain(llm=llm)\n",
    "check_query_chain = requires_decomposition(llm=llm)\n",
    "rephraser_chain = rephrase_chain(llm=llm)\n",
    "planner_chain = get_plan_chain(llm=llm)\n",
    "\n",
    "check_query_chain.invoke('what is fully homomorphic encryption?')\n",
    "print(rephraser_chain.invoke('what is fully homomorphic encryption?'))\n",
    "\n",
    "print(decomposer_chain.invoke('what is fully homomorphic encryption?, where is it used, and what are the latest research'))\n",
    "p= (planner_chain.invoke({'task': 'what is fully homomorphic encryption?', 'messages': []}))\n",
    "print(p)\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "\n",
    "\n",
    "matches = re.findall(regex_pattern, p)\n",
    "matches\n",
    "# planner_chain.invoke({'task': 'what is positional embedding', 'messages': list(ResearchTeamState['messages'])})\n",
    "class ResearchTeamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]    \n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str\n",
    "def decompose_or_rephrase(state: ResearchTeamState):\n",
    "    question = state['messages'][-1].content\n",
    "    task = None\n",
    "    try:\n",
    "        status = check_query_chain.invoke(question)\n",
    "\n",
    "        if status == 'Decompose':\n",
    "            task = decomposer_chain.invoke(question)\n",
    "        elif status == 'Rephrase':\n",
    "            task = rephraser_chain.invoke(question)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected status from check_query_chain: {status}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during processing: {e}\")\n",
    "\n",
    "    return {'task': task}\n",
    "\n",
    "\n",
    "def get_plan(state: ResearchTeamState):\n",
    "    regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "\n",
    "    task = state['task']\n",
    "    messages = state['messages']\n",
    "    plan = planner_chain.invoke({'task': task, 'messages': messages})\n",
    "\n",
    "    matches = re.findall(regex_pattern, plan)\n",
    "    return {\"steps\": matches, \"plan\": plan}\n",
    "\n",
    "\n",
    "\n",
    "rag_agent = create_react_agent(llm, tools=[qdrant_retriever_tool], checkpointer=memory)\n",
    "search_agent = create_react_agent(llm, tools=[tavily_search_tool, arxiv_search_tool])\n",
    "code_agent = create_react_agent(llm, tools=[repl_tool])\n",
    "\n",
    "\n",
    "def rag_node(state: ResearchTeamState):\n",
    "    trimmed_state_messages = state['messages']\n",
    "    return {\"messages\": [HumanMessage(content=trimmed_state_messages[-1].content, name=\"RagSearcher\")]}\n",
    "\n",
    "def code_node(state: ResearchTeamState):\n",
    "    trimmed_state_messages = state['messages']\n",
    "    return {\"messages\": [HumanMessage(content=trimmed_state_messages[-1].content, name=\"Coder\")]}\n",
    "\n",
    "\n",
    "def search_node(state: ResearchTeamState):\n",
    "    trimmed_state_messages = state['messages']\n",
    "    return {\"messages\": [HumanMessage(content=trimmed_state_messages[-1].content, name=\"Searcher\")]}\n",
    "\n",
    "def _get_current_task(state: ResearchTeamState):\n",
    "    # Ensure \"steps\" is defined and not empty\n",
    "    if \"steps\" not in state or not state[\"steps\"]:\n",
    "        raise ValueError(\"The 'steps' list in the state is empty or undefined.\")\n",
    "\n",
    "    # Handle cases where \"results\" is not present or empty\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "\n",
    "    # Ensure the results list is valid\n",
    "    if not isinstance(state[\"results\"], dict):\n",
    "        raise ValueError(\"'results' should be a dictionary containing step outputs.\")\n",
    "\n",
    "    # If all steps are completed, return -1 as an explicit indicator\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return -1  # Explicitly indicate completion\n",
    "\n",
    "    # Return the index of the next task\n",
    "    return len(state[\"results\"]) + 1\n",
    "\n",
    "\n",
    "\n",
    "def agent_exec(state: ResearchTeamState):\n",
    "    \"\"\"Worker node that executes the agents accordingly for a given plan.\"\"\"\n",
    "\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    _step = _get_current_task(state)\n",
    "    step_desc, step_name, agent, agent_input = state[\"steps\"][_step - 1]\n",
    "\n",
    "    for k, v in _results.items():\n",
    "        agent_input = agent_input.replace(k, v)\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if agent == \"RagSearcher\":\n",
    "        result = rag_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    elif agent == \"Searcher\":\n",
    "        result = search_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    elif agent == \"ChatBot\":\n",
    "        result = llm.invoke(agent_input)\n",
    "    elif agent == \"Coder\":\n",
    "        result = code_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent}\")\n",
    "\n",
    "    if result is None:\n",
    "        raise ValueError(f\"Agent {agent} did not return a result for step {step_name}\")\n",
    "\n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}\n",
    "\n",
    "solve_prompt = \"\"\"\n",
    "We have created a detailed step-by-step Plan to solve the given task and obtained corresponding answers from agents for each step in the Plan. \n",
    "Use these agent-provided answers as Evidence to craft a clear, comprehensive, and cohesive response.\n",
    "\n",
    "Plan:  \n",
    "{plan}\n",
    "\n",
    "Using the Evidence from the answers provided for each step in the Plan, solve the given task:  \n",
    "Task: {task}\n",
    "\n",
    "Provide your response below in a well-structured and coherent format:  \n",
    "Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ResearchTeamState):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, agent, agent_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            agent_input = agent_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {agent}[{agent_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content, 'messages': [result]}\n",
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        return \"agent_exec\"\n",
    "# def enter_chain(human_message: str):\n",
    "#     entry_syntax = {\"messages\": [HumanMessage(content=human_message)],}\n",
    "#     return entry_syntax\n",
    "\n",
    "\n",
    "graph = StateGraph(ResearchTeamState)\n",
    "\n",
    "graph.add_node('decompose_or_rephrase', decompose_or_rephrase)\n",
    "graph.add_node('get_plan', get_plan)\n",
    "graph.add_node(\"agent_exec\", agent_exec)\n",
    "graph.add_node(\"solve\", solve)\n",
    "\n",
    "graph.add_edge(START, 'decompose_or_rephrase')\n",
    "graph.add_edge('decompose_or_rephrase', 'get_plan')\n",
    "graph.add_edge('get_plan', 'agent_exec')\n",
    "graph.add_conditional_edges(\"agent_exec\", _route)\n",
    "graph.add_edge(\"solve\", END)\n",
    "\n",
    "research_graph_compiled = graph.compile(checkpointer=memory)\n",
    "research_graph = research_graph_compiled\n",
    "# from IPython.display import display, Image\n",
    "# display(Image(research_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for s in research_graph.stream({\"messages\": [HumanMessage(content='what is fully homomorphic encryption.')]}, config=config,):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "\n",
    "# print(s[\"solve\"][\"result\"])\n",
    "# # for s in rag_agent.stream({'messages': [HumanMessage(content='what is the main topic or subject of san.pdf?')]}):\n",
    "# #     print(s)\n",
    "# #     print('----')\n",
    "# r = research_graph.get_state(config=config, subgraphs=True)\n",
    "# r.values['messages']\n",
    "# print(r.values['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get current user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_user(request: Request):\n",
    "    # Retrieve the token from cookies\n",
    "    token = request.cookies.get(\"auth_token\")\n",
    "    if not token:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Not authenticated\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Decode the JWT token with automatic expiration verification\n",
    "        payload = decode(\n",
    "            token, SECRET_KEY, algorithms=[ALGORITHM], options={\"verify_exp\": True}\n",
    "        )\n",
    "        user_id: str = payload.get(\"user_id\")\n",
    "        email: str = payload.get(\"email\")\n",
    "\n",
    "        # Validate the presence of user_id and email\n",
    "        if not user_id or not email:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                detail=\"Invalid token: missing user_id or email\",\n",
    "                headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "            )\n",
    "\n",
    "        # Return user details as a structured response\n",
    "        return {\"user_id\": user_id, \"email\": email}\n",
    "\n",
    "    except ExpiredSignatureError:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Token has expired\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )\n",
    "    except InvalidTokenError:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Token is invalid\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=f\"Error decoding token: {str(e)}\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# middleware code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UserAuthMiddleware class\n",
    "class UserAuthMiddleware(BaseHTTPMiddleware):\n",
    "    def __init__(self, app, secret_key: str, algorithm: str):\n",
    "        super().__init__(app)\n",
    "        self.secret_key = secret_key\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        unprotected_routes = [\n",
    "            \"/api/login\",\n",
    "            \"/api/signup\",\n",
    "            \"/api/logout\",\n",
    "            \"/api/docs\",\n",
    "            \"/api/openapi.json\",\n",
    "        ]\n",
    "\n",
    "        # Allow unprotected routes without authentication\n",
    "        for route in unprotected_routes:\n",
    "            if re.fullmatch(route, request.url.path):\n",
    "                response = await call_next(request)\n",
    "                return response\n",
    "\n",
    "        # Example regex for routes to skip authentication\n",
    "        # if re.fullmatch(r\"/api/.*/\\w+\", request.url.path):\n",
    "        if re.fullmatch(r\"/api/[\\w-]+/\\{([\\w-]+)\\}(/[\\w.-]+)?\", request.url.path):\n",
    "            response = await call_next(request)\n",
    "            return response\n",
    "\n",
    "        # Extract token from cookies\n",
    "        token = request.cookies.get(\"auth_token\")\n",
    "        if not token:\n",
    "            raise HTTPException(status_code=401, detail=\"Not authenticated\")\n",
    "\n",
    "        try:\n",
    "            payload = decode(token, self.secret_key, algorithms=[self.algorithm])\n",
    "            exp = payload.get(\"exp\")\n",
    "            if exp and datetime.utcfromtimestamp(exp) < datetime.utcnow():\n",
    "                raise HTTPException(status_code=401, detail=\"Token has expired\")\n",
    "\n",
    "            # Add user data to request state\n",
    "            user = {\n",
    "                \"user_id\": payload.get(\"user_id\"),\n",
    "                \"email\": payload.get(\"email\"),\n",
    "            }\n",
    "            request.state.user = user\n",
    "\n",
    "        except ExpiredSignatureError as e:\n",
    "            raise HTTPException(status_code=401, detail=f\"Token expired: {str(e)}\")\n",
    "        except InvalidTokenError as e:\n",
    "            raise HTTPException(status_code=401, detail=f\"Invalid token: {str(e)}\")\n",
    "        except DecodeError as e:\n",
    "            raise HTTPException(status_code=400, detail=f\"Malformed token: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "        # Continue to the next middleware or route handler\n",
    "        response = await call_next(request)\n",
    "        return response\n",
    "\n",
    "# Add UserAuthMiddleware to the application\n",
    "app.add_middleware(\n",
    "    UserAuthMiddleware,\n",
    "    secret_key=SECRET_KEY,\n",
    "    algorithm=ALGORITHM,\n",
    ")\n",
    "\n",
    "\n",
    "def get_authenticated_user(request: Request):\n",
    "    if not request.state.user:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Not authenticated\",\n",
    "        )\n",
    "    return request.state.user\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

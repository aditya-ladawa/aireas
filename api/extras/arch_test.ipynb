{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, Optional, Union, TypedDict, Literal, Dict, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, trim_messages, AIMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "from team_tools import tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool\n",
    "\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "from chains import decomposition_chain, requires_decomposition, rephrase_chain\n",
    "from typing import Annotated, List, Union, TypedDict\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview')\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=120000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles\",\n",
    ")\n",
    "\n",
    "dec = decomposition_chain(llm=llm)\n",
    "rdec = requires_decomposition(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of energy?, What is meant by positive in the context of energy?, How does positive energy manifest or impact a system or individual?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.invoke('what is positive energy?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewoo_prompt = \"\"\"\n",
    "You will be given a string of simple question or questions seperated by comma. For each question, develop a series of sequential plans that specify exactly which agents to use to retrieve the necessary evidence. Format each plan in the following way:\n",
    "\n",
    "Format:\n",
    "Plan: [Provide a concise description of the intended action, including any specific sources, search queries, or steps that must be followed. Reference any evidence needed from previous steps.]\n",
    "#E[number] = [Agent[Specific Query/Input, including any references to previous #E results if applicable]]\n",
    "\n",
    "Use the minimum number of plans necessary to provide an accurate and relevant answer. Each plan should be followed by only one #E, with clear sequential ordering for reference by subsequent steps.\n",
    "Strictly provide output only. Provide a complete plan that addresses all questions as a whole, instead of creating individual plans for each question.\n",
    "\n",
    "Agents Available:\n",
    "- RagSearcher[input]: Uses a vector database to retrieve relevant documents or research papers based on prior knowledge or pre-embedded content (use for tasks related to research papers in the form of PDFs or topics embedded in the database).\n",
    "- Searcher[input]: Conducts searches via Tavily search, Web Scraper, or Arxiv Search to retrieve both general web information and academic papers from online sources.\n",
    "- Coder[input]: A code-execution agent using Python REPL for tasks requiring code, data analysis, or visualizations.\n",
    "- ChatBot[input]: Processes or generates natural language responses based on gathered evidence or specific input.\n",
    "\n",
    "### Routing Guidance:\n",
    "- For research papers or queries based on prior knowledge, use **RagSearcher** to pull from the vector database.\n",
    "- For general online or web-based searches, use **Searcher**.\n",
    "- For code or analysis tasks, use **Coder**.\n",
    "- For summarization or extracting specific information from prior steps, use **ChatBot**.\n",
    "\n",
    "Embedded Documents in the Vector Database:\n",
    "   - \"Attention is All You Need\" (name: a.pdf)\n",
    "   - \"MAGVIT: Masked Generative Video Transformer\" (name: m.pdf)\n",
    "   - \"SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer\" (name: san.pdf)\n",
    "\n",
    "### Advice:\n",
    "- When creating each plan, ensure that the query given to the agent precisely reflects the questions received. This will help the agent retrieve the most relevant answer directly aligned with the original question.\n",
    "\n",
    "### Examples\n",
    "Task: Summarize recent advancements in Video Transformers for action recognition tasks.\n",
    "Plan: Search for recent publications on Arxiv that discuss Video Transformers for action recognition using the Searcher agent. #E1 = Searcher[Video Transformers action recognition]\n",
    "Plan: Retrieve related research papers on Video Transformers stored in the vector database using the RagSearcher agent. #E2 = RagSearcher[Video Transformers action recognition]\n",
    "Plan: Use the retrieved documents to generate a summary highlighting advancements in the use of Video Transformers for action recognition. #E3 = ChatBot[Summarize #E1, #E2]\n",
    "\n",
    "Task: Analyze the importance of GAN metrizability for improved performance in generative models.\n",
    "Plan: Search Arxiv for recent studies on GAN metrizability and its impact on generative model performance using the Searcher agent. #E1 = Searcher[GAN metrizability and generative models]\n",
    "Plan: Retrieve any embedded research on GAN metrizability from the vector database using the RagSearcher agent. #E2 = RagSearcher[GAN metrizability in generative models]\n",
    "Plan: Summarize findings on why metrizability is significant for GAN performance based on the retrieved papers. #E3 = ChatBot[Summarize #E1, #E2]\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rewoo_solve_prompt = \"\"\"Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer\n",
    "directly with no extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ResearchTeamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is magvit?, what is the abstract of san.pdf?, what are recent research papers on video transformers?\n"
     ]
    }
   ],
   "source": [
    "task = dec.invoke('abstract of san.pdf, what is Magvit? and what are recent research papwers on video trnaformers/').lower()\n",
    "print(task)\n",
    "\n",
    "# task = dec.invoke('abstract of san.pdf').lower()\n",
    "# print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Regex to match expressions of the form E#... = ...[...]\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", rewoo_prompt)])\n",
    "planner = prompt_template | llm\n",
    "\n",
    "\n",
    "def get_plan(state: ResearchTeamState):\n",
    "    task = state[\"task\"]\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    # Find all matches in the sample text\n",
    "    matches = re.findall(regex_pattern, result.content)\n",
    "    return {\"steps\": matches, \"plan_string\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan: Retrieve the abstract of the embedded \"SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer\" research paper from the vector database using the RagSearcher agent. #E1 = RagSearcher[abstract of san.pdf]\n",
      "\n",
      "Plan: Use the retrieved abstract to understand the problem of metrizability and identify potential solutions discussed in the paper. #E2 = ChatBot[Extract problem and solutions from #E1]\n",
      "\n",
      "Plan: Search for additional research on solving metrizability problems in GANs using the Searcher agent. #E3 = Searcher[machine learning models for solving metrizability problem]\n",
      "\n",
      "Plan: Analyze the retrieved information to provide a comprehensive answer on how to solve the problem of metrizability. #E4 = ChatBot[Combine #E2 and #E3 to provide an answer]\n"
     ]
    }
   ],
   "source": [
    "print(planner.invoke('abstract of san.pdf, how can we solve the problem of metriziability').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = create_react_agent(llm, tools=[qdrant_retriever_tool])\n",
    "\n",
    "search_agent = create_react_agent(llm, tools=[tavily_search_tool, arxiv_search_tool])\n",
    "\n",
    "code_agent = create_react_agent(llm, tools=[repl_tool])\n",
    "\n",
    "def chatbot(state: ResearchTeamState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1].content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_current_task(state: ResearchTeamState):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "\n",
    "def tool_execution(state: ResearchTeamState):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    _step = _get_current_task(state)\n",
    "    step_desc, step_name, agent, agent_input = state[\"steps\"][_step - 1]\n",
    "    for k, v in _results.items():\n",
    "        agent_input = agent_input.replace(k, v)\n",
    "\n",
    "    # Initialize `result` to a default value\n",
    "    result = None\n",
    "\n",
    "    if agent == \"RagSearcher\":\n",
    "        result = rag_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    elif agent == \"Searcher\":\n",
    "        result = search_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    elif agent == \"ChatBot\":\n",
    "        result = llm.invoke(agent_input)\n",
    "    elif agent == \"Coder\":\n",
    "        result = code_agent.invoke({\"messages\": [HumanMessage(content=agent_input)]})['messages'][-1].content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent}\")\n",
    "\n",
    "    # Ensure `result` is not None before using it\n",
    "    if result is None:\n",
    "        raise ValueError(f\"Agent {agent} did not return a result for step {step_name}\")\n",
    "\n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_prompt = \"\"\"SSolve the following task or problem. \n",
    "To solve the problem, we have made a step-by-step Plan and retrieved corresponding Evidence for each step. \n",
    "Use the Evidence with caution, as long evidence might contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to the provided Evidence above. For each subquestion, respond explicitly in a Q&A format. \n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Response:\n",
    "Q1: [Subquestion 1]  \n",
    "A1: [Answer to Subquestion 1 based on relevant Evidence]  \n",
    "\n",
    "... (and so on for all subquestions)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ResearchTeamState):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, agent, agent_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            agent_input = agent_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {agent}[{agent_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content, 'messages': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "graph = StateGraph(ResearchTeamState)\n",
    "graph.add_node(\"plan\", get_plan)\n",
    "graph.add_node(\"tool\", tool_execution)\n",
    "graph.add_node(\"solve\", solve)\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': {'plan_string': 'Plan: Utilize the Coder agent to generate Python code for visualizing attention. #E1 = Coder[Python code for visualizing attention]\\n\\nPlan: Retrieve the research paper \"Attention is All You Need\" from the vector database using the RagSearcher agent. #E2 = RagSearcher[contents of a.pdf]\\n\\nPlan: Summarize the key components of the attention mechanism as described in the retrieved paper. #E3 = ChatBot[Summarize #E2]\\n\\nPlan: Refine the Python code to visualize attention based on the summarized components from the retrieved paper. #E4 = Coder[Visualize attention using components from #E3]\\n\\nPlan: Provide the final code snippet and its explanation for visualizing attention. #E5 = ChatBot[Explain #E4]', 'steps': [('Utilize the Coder agent to generate Python code for visualizing attention. ', '#E1', 'Coder', 'Python code for visualizing attention'), ('Retrieve the research paper \"Attention is All You Need\" from the vector database using the RagSearcher agent. ', '#E2', 'RagSearcher', 'contents of a.pdf'), ('Summarize the key components of the attention mechanism as described in the retrieved paper. ', '#E3', 'ChatBot', 'Summarize #E2'), ('Refine the Python code to visualize attention based on the summarized components from the retrieved paper. ', '#E4', 'Coder', 'Visualize attention using components from #E3'), ('Provide the final code snippet and its explanation for visualizing attention. ', '#E5', 'ChatBot', 'Explain #E4')]}}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool': {'results': {'#E1': 'This code will generate a random 10x10 attention matrix and display it as a heatmap. The heatmap will have a color bar on the right side, which represents the intensity of the attention. The `cmap=\"hot\"` argument makes the heatmap use a color scheme where higher values are represented by hotter colors (i.e., colors closer to red). The `interpolation=\"nearest\"` argument makes the heatmap use the nearest-neighbor interpolation method, which can make the heatmap look more pixelated.\\n\\nYou can replace the random attention matrix with your own attention matrix to visualize it.'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'This code will generate a random 10x10 attention matrix and display it as a heatmap. The heatmap will have a color bar on the right side, which represents the intensity of the attention. The `cmap=\"hot\"` argument makes the heatmap use a color scheme where higher values are represented by hotter colors (i.e., colors closer to red). The `interpolation=\"nearest\"` argument makes the heatmap use the nearest-neighbor interpolation method, which can make the heatmap look more pixelated.\\n\\nYou can replace the random attention matrix with your own attention matrix to visualize it.', '#E2': 'The function `retrieve_research_paper_texts` was called with the argument `\"contents of a.pdf\"`. The function returned a list of objects, each containing a `pdf_id` and a `page_content`. The `page_content` appears to be a snippet of text from a research paper.\\n\\nThe text snippet includes citations and references to various research papers and authors, as well as some mathematical notation and terminology related to machine learning and generative adversarial networks (GANs).\\n\\nOne of the references is to a paper titled \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" by Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Another reference is to a paper titled \"Improved techniques for training GANs\" by Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\n\\nThe text also mentions various concepts and techniques related to GANs, including sliced optimal transport, Wasserstein distance, and functional mean divergence.\\n\\nOverall, the text appears to be a snippet from a research paper on machine learning and GANs, and the function `retrieve_research_paper_texts` was able to retrieve relevant information from the vector database containing texts of research papers and scholarly articles.'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'This code will generate a random 10x10 attention matrix and display it as a heatmap. The heatmap will have a color bar on the right side, which represents the intensity of the attention. The `cmap=\"hot\"` argument makes the heatmap use a color scheme where higher values are represented by hotter colors (i.e., colors closer to red). The `interpolation=\"nearest\"` argument makes the heatmap use the nearest-neighbor interpolation method, which can make the heatmap look more pixelated.\\n\\nYou can replace the random attention matrix with your own attention matrix to visualize it.', '#E2': 'The function `retrieve_research_paper_texts` was called with the argument `\"contents of a.pdf\"`. The function returned a list of objects, each containing a `pdf_id` and a `page_content`. The `page_content` appears to be a snippet of text from a research paper.\\n\\nThe text snippet includes citations and references to various research papers and authors, as well as some mathematical notation and terminology related to machine learning and generative adversarial networks (GANs).\\n\\nOne of the references is to a paper titled \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" by Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Another reference is to a paper titled \"Improved techniques for training GANs\" by Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\n\\nThe text also mentions various concepts and techniques related to GANs, including sliced optimal transport, Wasserstein distance, and functional mean divergence.\\n\\nOverall, the text appears to be a snippet from a research paper on machine learning and GANs, and the function `retrieve_research_paper_texts` was able to retrieve relevant information from the vector database containing texts of research papers and scholarly articles.', '#E3': 'content=\\'The function `retrieve_research_paper_texts` was called with an argument `\"contents of a.pdf\"`, which suggests that the function was given a PDF research paper to analyze. The function successfully returned a list of objects, each containing a `pdf_id` and a `page_content`, which includes snippets of text from the research paper.\\\\n\\\\nThe text snippets appear to be from a research paper on machine learning, specifically generative adversarial networks (GANs). The text mentions several key concepts and techniques, such as sliced optimal transport, Wasserstein distance, and functional mean divergence.\\\\n\\\\nAdditionally, the text includes references to other research papers, including \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" and \"Improved techniques for training GANs\". These references suggest that the paper is engaging with existing research in the field of GANs.\\\\n\\\\nOverall, the function `retrieve_research_paper_texts` successfully extracted relevant information from the research paper, including text snippets, citations, and references. This information could be useful for further analysis or research on machine learning and GANs.\\' additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 225, \\'prompt_tokens\\': 315, \\'total_tokens\\': 540, \\'completion_time\\': 0.841443168, \\'prompt_time\\': 0.040782188, \\'queue_time\\': 0.005647382000000006, \\'total_time\\': 0.882225356}, \\'model_name\\': \\'llama-3.2-90b-vision-preview\\', \\'system_fingerprint\\': \\'fp_9c2a937c92\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None} id=\\'run-852cd8da-176b-48ac-abf0-60913a866abf-0\\' usage_metadata={\\'input_tokens\\': 315, \\'output_tokens\\': 225, \\'total_tokens\\': 540}'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'This code will generate a random 10x10 attention matrix and display it as a heatmap. The heatmap will have a color bar on the right side, which represents the intensity of the attention. The `cmap=\"hot\"` argument makes the heatmap use a color scheme where higher values are represented by hotter colors (i.e., colors closer to red). The `interpolation=\"nearest\"` argument makes the heatmap use the nearest-neighbor interpolation method, which can make the heatmap look more pixelated.\\n\\nYou can replace the random attention matrix with your own attention matrix to visualize it.', '#E2': 'The function `retrieve_research_paper_texts` was called with the argument `\"contents of a.pdf\"`. The function returned a list of objects, each containing a `pdf_id` and a `page_content`. The `page_content` appears to be a snippet of text from a research paper.\\n\\nThe text snippet includes citations and references to various research papers and authors, as well as some mathematical notation and terminology related to machine learning and generative adversarial networks (GANs).\\n\\nOne of the references is to a paper titled \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" by Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Another reference is to a paper titled \"Improved techniques for training GANs\" by Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\n\\nThe text also mentions various concepts and techniques related to GANs, including sliced optimal transport, Wasserstein distance, and functional mean divergence.\\n\\nOverall, the text appears to be a snippet from a research paper on machine learning and GANs, and the function `retrieve_research_paper_texts` was able to retrieve relevant information from the vector database containing texts of research papers and scholarly articles.', '#E3': 'content=\\'The function `retrieve_research_paper_texts` was called with an argument `\"contents of a.pdf\"`, which suggests that the function was given a PDF research paper to analyze. The function successfully returned a list of objects, each containing a `pdf_id` and a `page_content`, which includes snippets of text from the research paper.\\\\n\\\\nThe text snippets appear to be from a research paper on machine learning, specifically generative adversarial networks (GANs). The text mentions several key concepts and techniques, such as sliced optimal transport, Wasserstein distance, and functional mean divergence.\\\\n\\\\nAdditionally, the text includes references to other research papers, including \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" and \"Improved techniques for training GANs\". These references suggest that the paper is engaging with existing research in the field of GANs.\\\\n\\\\nOverall, the function `retrieve_research_paper_texts` successfully extracted relevant information from the research paper, including text snippets, citations, and references. This information could be useful for further analysis or research on machine learning and GANs.\\' additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 225, \\'prompt_tokens\\': 315, \\'total_tokens\\': 540, \\'completion_time\\': 0.841443168, \\'prompt_time\\': 0.040782188, \\'queue_time\\': 0.005647382000000006, \\'total_time\\': 0.882225356}, \\'model_name\\': \\'llama-3.2-90b-vision-preview\\', \\'system_fingerprint\\': \\'fp_9c2a937c92\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None} id=\\'run-852cd8da-176b-48ac-abf0-60913a866abf-0\\' usage_metadata={\\'input_tokens\\': 315, \\'output_tokens\\': 225, \\'total_tokens\\': 540}', '#E4': '<function=python_repl>\"code = \\\\\"import matplotlib.pyplot as plt\\\\nimport networkx as nx\\\\nimport numpy as np\\\\n\\\\n# Define the nodes\\\\nnodes = [\\'retrieve_research_paper_texts\\', \\'machine learning\\', \\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\', \\'Wasserstein distance\\', \\'functional mean divergence\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\', \\'Improved techniques for training GANs\\']\\\\n\\\\n# Define the edges\\\\nedges = [(\\'retrieve_research_paper_texts\\', \\'machine learning\\'), (\\'retrieve_research_paper_texts\\', \\'generative adversarial networks (GANs)\\'), (\\'machine learning\\', \\'generative adversarial networks (GANs)\\'), (\\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\'), (\\'generative adversarial networks (GANs)\\', \\'Wasserstein distance\\'), (\\'generative adversarial networks (GANs)\\', \\'functional mean divergence\\'), (\\'generative adversarial networks (GANs)\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\'), (\\'generative adversarial networks (GANs)\\', \\'Improved techniques for training GANs\\')]\\\\n\\\\n# Create the graph\\\\nG = nx.Graph()\\\\nG.add_nodes_from(nodes)\\\\nG.add_edges_from(edges)\\\\n\\\\n# Position the nodes\\\\npos = nx.spring_layout(G)\\\\n\\\\n# Draw the nodes and edges\\\\nnx.draw_networkx_nodes(G, pos, node_size=7000, node_color=\\'lightblue\\')\\\\nnx.draw_networkx_edges(G, pos, width=2, edge_color=\\'gray\\')\\\\nnx.draw_networkx_labels(G, pos, font_size=10)\\\\n\\\\n# Show the plot\\\\nplt.show()\\\\\"\"'}}}\n",
      "---\n",
      "{'tool': {'results': {'#E1': 'This code will generate a random 10x10 attention matrix and display it as a heatmap. The heatmap will have a color bar on the right side, which represents the intensity of the attention. The `cmap=\"hot\"` argument makes the heatmap use a color scheme where higher values are represented by hotter colors (i.e., colors closer to red). The `interpolation=\"nearest\"` argument makes the heatmap use the nearest-neighbor interpolation method, which can make the heatmap look more pixelated.\\n\\nYou can replace the random attention matrix with your own attention matrix to visualize it.', '#E2': 'The function `retrieve_research_paper_texts` was called with the argument `\"contents of a.pdf\"`. The function returned a list of objects, each containing a `pdf_id` and a `page_content`. The `page_content` appears to be a snippet of text from a research paper.\\n\\nThe text snippet includes citations and references to various research papers and authors, as well as some mathematical notation and terminology related to machine learning and generative adversarial networks (GANs).\\n\\nOne of the references is to a paper titled \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" by Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Another reference is to a paper titled \"Improved techniques for training GANs\" by Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\n\\nThe text also mentions various concepts and techniques related to GANs, including sliced optimal transport, Wasserstein distance, and functional mean divergence.\\n\\nOverall, the text appears to be a snippet from a research paper on machine learning and GANs, and the function `retrieve_research_paper_texts` was able to retrieve relevant information from the vector database containing texts of research papers and scholarly articles.', '#E3': 'content=\\'The function `retrieve_research_paper_texts` was called with an argument `\"contents of a.pdf\"`, which suggests that the function was given a PDF research paper to analyze. The function successfully returned a list of objects, each containing a `pdf_id` and a `page_content`, which includes snippets of text from the research paper.\\\\n\\\\nThe text snippets appear to be from a research paper on machine learning, specifically generative adversarial networks (GANs). The text mentions several key concepts and techniques, such as sliced optimal transport, Wasserstein distance, and functional mean divergence.\\\\n\\\\nAdditionally, the text includes references to other research papers, including \"Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\" and \"Improved techniques for training GANs\". These references suggest that the paper is engaging with existing research in the field of GANs.\\\\n\\\\nOverall, the function `retrieve_research_paper_texts` successfully extracted relevant information from the research paper, including text snippets, citations, and references. This information could be useful for further analysis or research on machine learning and GANs.\\' additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 225, \\'prompt_tokens\\': 315, \\'total_tokens\\': 540, \\'completion_time\\': 0.841443168, \\'prompt_time\\': 0.040782188, \\'queue_time\\': 0.005647382000000006, \\'total_time\\': 0.882225356}, \\'model_name\\': \\'llama-3.2-90b-vision-preview\\', \\'system_fingerprint\\': \\'fp_9c2a937c92\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None} id=\\'run-852cd8da-176b-48ac-abf0-60913a866abf-0\\' usage_metadata={\\'input_tokens\\': 315, \\'output_tokens\\': 225, \\'total_tokens\\': 540}', '#E4': '<function=python_repl>\"code = \\\\\"import matplotlib.pyplot as plt\\\\nimport networkx as nx\\\\nimport numpy as np\\\\n\\\\n# Define the nodes\\\\nnodes = [\\'retrieve_research_paper_texts\\', \\'machine learning\\', \\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\', \\'Wasserstein distance\\', \\'functional mean divergence\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\', \\'Improved techniques for training GANs\\']\\\\n\\\\n# Define the edges\\\\nedges = [(\\'retrieve_research_paper_texts\\', \\'machine learning\\'), (\\'retrieve_research_paper_texts\\', \\'generative adversarial networks (GANs)\\'), (\\'machine learning\\', \\'generative adversarial networks (GANs)\\'), (\\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\'), (\\'generative adversarial networks (GANs)\\', \\'Wasserstein distance\\'), (\\'generative adversarial networks (GANs)\\', \\'functional mean divergence\\'), (\\'generative adversarial networks (GANs)\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\'), (\\'generative adversarial networks (GANs)\\', \\'Improved techniques for training GANs\\')]\\\\n\\\\n# Create the graph\\\\nG = nx.Graph()\\\\nG.add_nodes_from(nodes)\\\\nG.add_edges_from(edges)\\\\n\\\\n# Position the nodes\\\\npos = nx.spring_layout(G)\\\\n\\\\n# Draw the nodes and edges\\\\nnx.draw_networkx_nodes(G, pos, node_size=7000, node_color=\\'lightblue\\')\\\\nnx.draw_networkx_edges(G, pos, width=2, edge_color=\\'gray\\')\\\\nnx.draw_networkx_labels(G, pos, font_size=10)\\\\n\\\\n# Show the plot\\\\nplt.show()\\\\\"\"', '#E5': 'content=\"**NetworkX and Matplotlib Code Explanation**\\\\n\\\\nThis Python code is used to create a simple graph visualization using the NetworkX library, with nodes and edges, and display it using Matplotlib. Here\\'s a breakdown of what each section does:\\\\n\\\\n### Importing Libraries\\\\n\\\\n```python\\\\nimport matplotlib.pyplot as plt\\\\nimport networkx as nx\\\\nimport numpy as np\\\\n```\\\\n\\\\nThis code imports the necessary libraries:\\\\n\\\\n* `matplotlib.pyplot` for creating static, animated, and interactive visualizations.\\\\n* `networkx` for creating, manipulating, and studying the structure, dynamics, and functions of complex networks.\\\\n* `numpy` is not actually used in this script, but it\\'s often imported when working with numerical computations.\\\\n\\\\n### Defining the Nodes and Edges\\\\n\\\\n```python\\\\n# Define the nodes\\\\nnodes = [\\'retrieve_research_paper_texts\\', \\'machine learning\\', \\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\', \\'Wasserstein distance\\', \\'functional mean divergence\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\', \\'Improved techniques for training GANs\\']\\\\n\\\\n# Define the edges\\\\nedges = [(\\'retrieve_research_paper_texts\\', \\'machine learning\\'), (\\'retrieve_research_paper_texts\\', \\'generative adversarial networks (GANs)\\'), (\\'machine learning\\', \\'generative adversarial networks (GANs)\\'), (\\'generative adversarial networks (GANs)\\', \\'sliced optimal transport\\'), (\\'generative adversarial networks (GANs)\\', \\'Wasserstein distance\\'), (\\'generative adversarial networks (GANs)\\', \\'functional mean divergence\\'), (\\'generative adversarial networks (GANs)\\', \\'Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis\\'), (\\'generative adversarial networks (GANs)\\', \\'Improved techniques for training GANs\\')]\\\\n```\\\\n\\\\nThis code defines the nodes and edges of the graph:\\\\n\\\\n* `nodes` is a list of strings representing the nodes in the graph.\\\\n* `edges` is a list of tuples, where each tuple represents a directed edge between two nodes.\\\\n\\\\n### Creating the Graph\\\\n\\\\n```python\\\\n# Create the graph\\\\nG = nx.Graph()\\\\nG.add_nodes_from(nodes)\\\\nG.add_edges_from(edges)\\\\n```\\\\n\\\\nThis code creates an empty graph using NetworkX and adds the nodes and edges to it:\\\\n\\\\n* `nx.Graph()` creates an empty undirected graph.\\\\n* `G.add_nodes_from(nodes)` adds the nodes to the graph.\\\\n* `G.add_edges_from(edges)` adds the edges to the graph.\\\\n\\\\n### Positioning the Nodes\\\\n\\\\n```python\\\\n# Position the nodes\\\\npos = nx.spring_layout(G)\\\\n```\\\\n\\\\nThis code positions the nodes in the graph using the Fruchterman-Reingold force-directed algorithm:\\\\n\\\\n* `nx.spring_layout(G)` calculates the positions of the nodes in the graph using the Fruchterman-Reingold algorithm.\\\\n\\\\n### Drawing the Nodes and Edges\\\\n\\\\n```python\\\\n# Draw the nodes and edges\\\\nnx.draw_networkx_nodes(G, pos, node_size=7000, node_color=\\'lightblue\\')\\\\nnx.draw_networkx_edges(G, pos, width=2, edge_color=\\'gray\\')\\\\nnx.draw_networkx_labels(G, pos, font_size=10)\\\\n```\\\\n\\\\nThis code draws the nodes and edges of the graph:\\\\n\\\\n* `nx.draw_networkx_nodes(G, pos, node_size=7000, node_color=\\'lightblue\\')` draws the nodes of the graph with a size of 7000 and a light blue color.\\\\n* `nx.draw_networkx_edges(G, pos, width=2, edge_color=\\'gray\\')` draws the edges of the graph with a width of 2 and a gray color.\\\\n* `nx.draw_networkx_labels(G, pos, font_size=10)` draws the labels of the nodes with a font size of 10.\\\\n\\\\n### Showing the Plot\\\\n\\\\n```python\\\\n# Show the plot\\\\nplt.show()\\\\n```\\\\n\\\\nThis code displays the graph visualization using Matplotlib:\\\\n\\\\n* `plt.show()` displays the graph visualization.\\\\n\\\\nWhen you run this code, it will display a graph visualization with nodes and edges, using the positions calculated by the Fruchterman-Reingold algorithm.\" additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 856, \\'prompt_tokens\\': 407, \\'total_tokens\\': 1263, \\'completion_time\\': 3.27120213, \\'prompt_time\\': 0.050863343, \\'queue_time\\': 0.005796016000000001, \\'total_time\\': 3.322065473}, \\'model_name\\': \\'llama-3.2-90b-vision-preview\\', \\'system_fingerprint\\': \\'fp_9c2a937c92\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None} id=\\'run-6da94af0-cacc-457a-89bd-3b3b68b898a5-0\\' usage_metadata={\\'input_tokens\\': 407, \\'output_tokens\\': 856, \\'total_tokens\\': 1263}'}}}\n",
      "---\n",
      "{'solve': {'result': '```python\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Generate a random 10x10 attention matrix\\nattention_matrix = np.random.rand(10, 10)\\n\\n# Create a heatmap of the attention matrix\\nplt.imshow(attention_matrix, cmap=\"hot\", interpolation=\"nearest\")\\n\\n# Add a color bar to the right side of the heatmap\\nplt.colorbar()\\n\\n# Show the plot\\nplt.show()\\n```'}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for s in app.stream({\"task\": 'can you write a dummy code to visualize attention'}):\n",
    "    print(s)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Attention, in the context of psychology and neuroscience, refers to the cognitive process of selectively concentrating on one aspect of the environment while ignoring other stimuli. It is a crucial mechanism that enables us to focus on relevant information, filter out distractions, and efficiently process sensory inputs.\\n\\nIn the context of the provided research paper, attention is used in a different sense. The paper discusses the Transformer model, a type of neural network architecture that relies entirely on attention mechanisms to draw global dependencies between input and output sequences. The model uses multi-head attention, which allows it to jointly attend to information from different representation subspaces at different positions.\\n\\nThe paper highlights the benefits of using attention mechanisms in sequence-to-sequence models, such as improved parallelization and reduced computational cost. It also discusses the applications of attention in the Transformer model, including encoder-decoder attention, self-attention, and position-wise feed-forward networks.\\n\\nOverall, attention is a critical component of the Transformer model, enabling it to efficiently process and generate sequences of data.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 2236, 'total_tokens': 2438, 'completion_time': 0.992718863, 'prompt_time': 0.3354443, 'queue_time': 0.007964397000000012, 'total_time': 1.328163163}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None}, id='run-3cda5b4e-6186-4a0d-b831-5d0b5e7e156d-0', usage_metadata={'input_tokens': 2236, 'output_tokens': 202, 'total_tokens': 2438})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for s in rag_agent.stream({\"messages\": [HumanMessage(content=\"What's attention\")]}):\n",
    "#     print(s)\n",
    "\n",
    "rag_agent.invoke({\"messages\": [HumanMessage(content=\"What's attention\")]})['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The difference between attention and self-attention is that attention is a mechanism that allows a model to focus on specific parts of the input data, while self-attention is a specific type of attention mechanism that allows a model to attend to different parts of the same input sequence.\\n\\nRecent research papers that apply transformer architectures, specifically video transformers, to video-related tasks include:\\n\\n* \"Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition\" by Hamid Mohammadi et al.\\n* \"Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding\" by Guo Chen et al.\\n* \"Towards Training Stronger Video Vision Transformers for EPIC-KITCHENS-100 Action Recognition\" by Ziyuan Huang et al.\\n* \"An End-to-End Trainable Video Panoptic Segmentation Method using Transformers\" by Jeongwon Ryu et al.\\n* \"Transformer-based Image and Video Inpainting: Current Challenges and Future Directions\" by Omar Elharrouss et al.\\n\\nThese papers demonstrate the effectiveness of transformer architectures in various video-related tasks, such as video violence recognition, action recognition, panoptic segmentation, and image and video inpainting.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 239, 'prompt_tokens': 3946, 'total_tokens': 4185, 'completion_time': 1.169977748, 'prompt_time': 0.575248224, 'queue_time': 0.008461520999999972, 'total_time': 1.745225972}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'stop', 'logprobs': None}, id='run-6eb310ad-e2ae-4494-bda0-b57eaf5d5469-0', usage_metadata={'input_tokens': 3946, 'output_tokens': 239, 'total_tokens': 4185})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for s in search_agent.stream({\"messages\": [HumanMessage(content='what are some recent research papers that apply transformer architectures, specifically video transformers, to video-related tasks?')]}):\n",
    "#     print(s)\n",
    "\n",
    "search_agent.invoke({\"messages\": [HumanMessage(content='whats te difference between attentiona nd self attention and what are some recent research papers that apply transformer architectures, specifically video transformers, to video-related tasks?')]})['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://home.cern/news/news/physics/still-making-tracks-eighty-years-positron',\n",
       "  'content': \"A positron is the antimatter counterpart of an electron, with opposite charge and identical mass. Learn how Carl Anderson discovered it in 1933, how it fits Dirac's prediction, and how it is used in particle physics and medicine.\"},\n",
       " {'url': 'https://byjus.com/chemistry/positron/',\n",
       "  'content': 'A positron is a positively charged particle that is the antiparticle of an electron. Learn how it is produced, how it interacts with matter and how it is used in antihydrogen research.'},\n",
       " {'url': 'https://www.allthescience.org/what-is-a-positron.htm',\n",
       "  'content': 'A positron is the antimatter equivalent of an electron, with a positive charge and a low mass. Learn about its discovery, properties, applications, and challenges in this article.'},\n",
       " {'url': 'https://www.vedantu.com/chemistry/positron',\n",
       "  'content': 'A positron or antielectron is the antimatter counterpart to an electron. A positron has the equal or same mass as an electron and a spin of 1/2, but it has an electrical charge of +1. When a positron collides with electron annihilation, it results in the production of two or more gamma-ray photons. Positron is also known as the positive'},\n",
       " {'url': 'https://pediaa.com/difference-between-proton-and-positron/',\n",
       "  'content': 'What is a Positron. Positron is a subatomic particle that is considered as an antielectron. It is the antiparticle of the electron. Therefore, the positron has a +1 electrical charge. The mass of positron is exactly equivalent to the mass of an electron; 9.1094 x 10-28 g. The atomic mass of positron is 0.00054858 amu.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_search_tool.invoke('what is positron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools=[qdrant_retriever_tool, web_scraper_tool, tavily_search_tool, arxiv_search_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7fqh', 'function': {'arguments': '{\"query\": \"self-attention\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 599, 'total_tokens': 619, 'completion_time': 0.097139516, 'prompt_time': 0.101830235, 'queue_time': 0.006444657999999992, 'total_time': 0.198969751}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-579f5bef-263e-42da-91b3-333b680ea46b-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'self-attention'}, 'id': 'call_7fqh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 599, 'output_tokens': 20, 'total_tokens': 619})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke('what is self attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pdf_id': 'a.pdf', '_id': '1313109e-8fe8-45cc-b249-a79821e51c1f', '_collection_name': 'aireas-cloud'}, page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is'),\n",
       " Document(metadata={'pdf_id': 'a.pdf', '_id': 'b2812931-4b32-407c-a5d0-bdd2860cfbd5', '_collection_name': 'aireas-cloud'}, page_content='computation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,'),\n",
       " Document(metadata={'pdf_id': 'a.pdf', '_id': 'b0536aa8-237f-46b9-b0d4-18f76cdc8baa', '_collection_name': 'aireas-cloud'}, page_content='i=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks'),\n",
       " Document(metadata={'pdf_id': 'a.pdf', '_id': '92ba5fc6-3f68-4a5c-b0cd-7fe72a0da157', '_collection_name': 'aireas-cloud'}, page_content='Since our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_retriever.invoke('self attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

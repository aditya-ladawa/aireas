{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Qdrant client.\n",
      "Collection 'aireas-cloud' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138179/1401692486.py:46: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  qdrant_retriever_tool = qdrant_retriever.as_tool(\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, List, Optional, Union, Dict, Annotated, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, trim_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# from team_tools import tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "# from llm_chains import decomposition_chain, requires_decomposition, rephrase_chain, get_plan_chain, assign_chat_topic\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from token_counter import tiktoken_counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview')\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=5984,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_chain = decomposition_chain(llm=llm)\n",
    "check_query_chain = requires_decomposition(llm=llm)\n",
    "rephraser_chain = rephrase_chain(llm=llm)\n",
    "planner_chain = get_plan_chain(llm=llm)\n",
    "assign_topic_chain = assign_chat_topic(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI-Generated Animation with Diffusion'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_topic_chain.invoke(' ill be researching on animation generation using AI, partial attention networks and incorporating diffusion models for frame gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = create_react_agent(model=llm, tools=[qdrant_retriever_tool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284})]}\n",
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='026e8fcc-61ca-42d2-9c2d-14a2cc48bc72', tool_call_id='call_8r5k', status='error')]}\n",
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='026e8fcc-61ca-42d2-9c2d-14a2cc48bc72', tool_call_id='call_8r5k', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znz3', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 314, 'total_tokens': 336, 'completion_time': 0.109683313, 'prompt_time': 0.053360303, 'queue_time': 0.009064937000000002, 'total_time': 0.163043616}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4cfa002a-c840-44d1-bd9d-87520abf5e0f-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_znz3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 22, 'total_tokens': 336})]}\n",
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='026e8fcc-61ca-42d2-9c2d-14a2cc48bc72', tool_call_id='call_8r5k', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znz3', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 314, 'total_tokens': 336, 'completion_time': 0.109683313, 'prompt_time': 0.053360303, 'queue_time': 0.009064937000000002, 'total_time': 0.163043616}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4cfa002a-c840-44d1-bd9d-87520abf5e0f-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_znz3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 22, 'total_tokens': 336}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='4629e4ac-38cb-41a4-981c-07bbcea61b5e', tool_call_id='call_znz3', status='error')]}\n",
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='026e8fcc-61ca-42d2-9c2d-14a2cc48bc72', tool_call_id='call_8r5k', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znz3', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 314, 'total_tokens': 336, 'completion_time': 0.109683313, 'prompt_time': 0.053360303, 'queue_time': 0.009064937000000002, 'total_time': 0.163043616}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4cfa002a-c840-44d1-bd9d-87520abf5e0f-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_znz3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 22, 'total_tokens': 336}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='4629e4ac-38cb-41a4-981c-07bbcea61b5e', tool_call_id='call_znz3', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cncs', 'function': {'arguments': '{\"__arg1\": \"main points of samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 366, 'total_tokens': 391, 'completion_time': 0.120593963, 'prompt_time': 0.06271774, 'queue_time': 0.007718241000000001, 'total_time': 0.183311703}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a41b2975-49b4-4cfe-b227-b188d9c127f2-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'main points of samurai.pdf'}, 'id': 'call_cncs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 366, 'output_tokens': 25, 'total_tokens': 391})]}\n",
      "{'messages': [HumanMessage(content='hello, what is samurai.pdf main points?', additional_kwargs={}, response_metadata={}, id='09ebad3a-739b-44bc-ac67-9388bb378092'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8r5k', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 262, 'total_tokens': 284, 'completion_time': 0.111734198, 'prompt_time': 0.050164637, 'queue_time': 0.008238533999999999, 'total_time': 0.161898835}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2bc75ad-c6b6-41c6-b097-39b29f9ddebb-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_8r5k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 262, 'output_tokens': 22, 'total_tokens': 284}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='026e8fcc-61ca-42d2-9c2d-14a2cc48bc72', tool_call_id='call_8r5k', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znz3', 'function': {'arguments': '{\"__arg1\": \"samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 314, 'total_tokens': 336, 'completion_time': 0.109683313, 'prompt_time': 0.053360303, 'queue_time': 0.009064937000000002, 'total_time': 0.163043616}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4cfa002a-c840-44d1-bd9d-87520abf5e0f-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'samurai.pdf'}, 'id': 'call_znz3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 22, 'total_tokens': 336}), ToolMessage(content=\"Error: ResponseHandlingException(ReadTimeout('The read operation timed out'))\\n Please fix your mistakes.\", name='retrieve_research_paper_texts', id='4629e4ac-38cb-41a4-981c-07bbcea61b5e', tool_call_id='call_znz3', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cncs', 'function': {'arguments': '{\"__arg1\": \"main points of samurai.pdf\"}', 'name': 'retrieve_research_paper_texts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 366, 'total_tokens': 391, 'completion_time': 0.120593963, 'prompt_time': 0.06271774, 'queue_time': 0.007718241000000001, 'total_time': 0.183311703}, 'model_name': 'llama-3.2-90b-vision-preview', 'system_fingerprint': 'fp_07b97e5459', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a41b2975-49b4-4cfe-b227-b188d9c127f2-0', tool_calls=[{'name': 'retrieve_research_paper_texts', 'args': {'__arg1': 'main points of samurai.pdf'}, 'id': 'call_cncs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 366, 'output_tokens': 25, 'total_tokens': 391}), ToolMessage(content=[], name='retrieve_research_paper_texts', id='06c2c694-b916-469c-9455-be0f8a754396', tool_call_id='call_cncs')]}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages.6' : for 'role:tool' the following must be satisfied[('messages.6.content' : value must be a string)]\", 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mastream({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello, what is samurai.pdf main points?\u001b[39m\u001b[38;5;124m'\u001b[39m)]}, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], HumanMessage):\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1538\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1533\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1534\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1535\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1536\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1537\u001b[0m ):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1539\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1540\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1541\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1542\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1543\u001b[0m     ):\n\u001b[1;32m   1544\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1546\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/runner.py:132\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    130\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/retry.py:102\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/utils/runnable.py:453\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/utils/runnable.py:224\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 224\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:569\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.acall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macall_model\u001b[39m(state: AgentState, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[0;32m--> 569\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model_runnable\u001b[38;5;241m.\u001b[39mainvoke(state, config)\n\u001b[1;32m    570\u001b[0m     has_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls\n\u001b[1;32m    571\u001b[0m     all_tools_return_direct \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28mall\u001b[39m(call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m should_return_direct \u001b[38;5;28;01mfor\u001b[39;00m call \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls)\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage)\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3066\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[0;32m-> 3066\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5366\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5360\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   5361\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5362\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5363\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   5367\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5369\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5370\u001b[0m     )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    306\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 307\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    308\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    790\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    795\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    797\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    798\u001b[0m     )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    746\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m             ]\n\u001b[1;32m    755\u001b[0m         )\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    757\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    758\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    760\u001b[0m ]\n\u001b[1;32m    761\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:924\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 924\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    925\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langchain_groq/chat_models.py:494\u001b[0m, in \u001b[0;36mChatGroq._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    490\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    493\u001b[0m }\n\u001b[0;32m--> 494\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/groq/resources/chat/completions.py:576\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    578\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    579\u001b[0m             {\n\u001b[1;32m    580\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    581\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    582\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    601\u001b[0m             },\n\u001b[1;32m    602\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    603\u001b[0m         ),\n\u001b[1;32m    604\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    605\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    606\u001b[0m         ),\n\u001b[1;32m    607\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    608\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    609\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m    610\u001b[0m     )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/groq/_base_client.py:1786\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1774\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1782\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1783\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1784\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/groq/_base_client.py:1494\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1487\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1493\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1495\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1496\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1497\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1498\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1499\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1500\u001b[0m     )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/groq/_base_client.py:1595\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1594\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1598\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1599\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1604\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages.6' : for 'role:tool' the following must be satisfied[('messages.6.content' : value must be a string)]\", 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "async for s in k.astream({'messages': [HumanMessage(content='hello, what is samurai.pdf main points?')]}, stream_mode='values'):\n",
    "            if isinstance(s['messages'][-1], HumanMessage):\n",
    "                continue\n",
    "            else:\n",
    "              print(s)\n",
    "\n",
    "            # Check if the AI calls a tool\n",
    "            # if 'tool_name' in msg:\n",
    "            #     tool_name = msg['tool_name']\n",
    "            #     query = msg.get('query', 'No query provided')\n",
    "\n",
    "            #     print(f\"\\n[AI is calling tool: {tool_name}]\")\n",
    "            #     print(f\"[Query sent to {tool_name}]: {query}\")\n",
    "\n",
    "            # # Check if the tool retrieved content\n",
    "            # elif 'content' in msg:\n",
    "            #     content = msg['content']\n",
    "            #     print(f\"\\n[Tool Retrieved Content]: {content}\")\n",
    "\n",
    "            # # Final Answer Display\n",
    "            # if 'final_answer' in msg:\n",
    "            #     print(f\"\\n[Final Answer]: {msg['final_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
